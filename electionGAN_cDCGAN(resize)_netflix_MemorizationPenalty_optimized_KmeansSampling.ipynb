{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87ed5f26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WIN10\\anaconda3\\envs\\tf1.12\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\WIN10\\anaconda3\\envs\\tf1.12\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\WIN10\\anaconda3\\envs\\tf1.12\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\WIN10\\anaconda3\\envs\\tf1.12\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\WIN10\\anaconda3\\envs\\tf1.12\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\WIN10\\anaconda3\\envs\\tf1.12\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os, time, itertools, imageio, pickle, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from utils import one_hot, resize_to_ori_calMRE, resize_to_ori\n",
    "from Kmeans_sampling import Kmeans\n",
    "\n",
    "tf.reset_default_graph() \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # select GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aea24567",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'election_GAN/'\n",
    "gen_results = 'generated_results/'\n",
    "\n",
    "model = 'model_'+'.ckpt'\n",
    "\n",
    "if not os.path.isdir(folder):\n",
    "    os.mkdir(folder)\n",
    "    \n",
    "if not os.path.isdir(folder + gen_results):\n",
    "    os.mkdir(folder + gen_results)\n",
    "\n",
    "# save ckpt\n",
    "saver_path = os.path.join(folder, model)\n",
    "\n",
    "# read ckpt\n",
    "restore_path = os.path.join(folder)\n",
    "\n",
    "# save generated data\n",
    "generated_path = os.path.join(folder + gen_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffeddad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 24, 30)\n",
      "(1000, 24, 30)\n",
      "(2000, 24, 30)\n"
     ]
    }
   ],
   "source": [
    "img_width = 30\n",
    "img_height = 24\n",
    "\n",
    "data_alt3_all = pd.read_csv('./data/netflix_data_3alt_resize.csv')\n",
    "data_alt3 = data_alt3_all.iloc[:1000,1:].values\n",
    "data_alt3_val = data_alt3_all.iloc[1000:2000,1:].values\n",
    "\n",
    "data_alt3 = data_alt3.reshape([-1, img_height, img_width])\n",
    "print(data_alt3.shape)\n",
    "\n",
    "##################################\n",
    "data_alt4_all = pd.read_csv('./data/netflix_data_4alt_resize.csv')\n",
    "data_alt4 = data_alt4_all.iloc[:1000,1:].values\n",
    "data_alt4_val = data_alt4_all.iloc[1000:2000,1:].values\n",
    "\n",
    "data_alt4 = data_alt4.reshape([-1, img_height, img_width])\n",
    "print(data_alt4.shape)\n",
    "\n",
    "data_alt_3_4 = np.concatenate( (data_alt3, data_alt4), axis = 0)\n",
    "print(data_alt_3_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f16811f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2)\n"
     ]
    }
   ],
   "source": [
    "label_alt3 = np.zeros([data_alt3.shape[0]])\n",
    "label_alt4 = np.zeros([data_alt4.shape[0]]) + 1\n",
    "label_alt_3_4 = np.concatenate( (label_alt3,label_alt4), axis = 0)\n",
    "\n",
    "label_alt_onehot = one_hot(label_alt_3_4, 1 + 1)   \n",
    "print(label_alt_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4647ae49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 6)\n",
      "(1000, 6)\n",
      "total used num_iter:  2\n"
     ]
    }
   ],
   "source": [
    "img_size = np.math.factorial(6)\n",
    "img_width = 30\n",
    "img_height = 24\n",
    "ori_size = np.math.factorial(3)\n",
    "\n",
    "data_alt3_ori = resize_to_ori_calMRE(data_alt3, img_size, img_width, img_height, ori_size)\n",
    "data_alt3_val_ori = resize_to_ori_calMRE(data_alt3_val, img_size, img_width, img_height, ori_size)\n",
    "print(data_alt3_ori.shape)\n",
    "print(data_alt3_val_ori.shape)\n",
    "\n",
    "model_alt3 = Kmeans(num_cluster = 10)\n",
    "model_alt3.fit(data_alt3_ori, num_alternative=3, max_iter = 1000)  #max_iter = 1000\n",
    "_, clustered_data_alt3 = model_alt3.predict_and_cluster(data_alt3_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d8135b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 24)\n",
      "(1000, 24)\n",
      "total used num_iter:  2\n"
     ]
    }
   ],
   "source": [
    "img_size = np.math.factorial(6)\n",
    "img_width = 30\n",
    "img_height = 24\n",
    "ori_size = np.math.factorial(4)\n",
    "\n",
    "data_alt4_ori = resize_to_ori_calMRE(data_alt4, img_size, img_width, img_height, ori_size)\n",
    "data_alt4_val_ori = resize_to_ori_calMRE(data_alt4_val, img_size, img_width, img_height, ori_size)\n",
    "print(data_alt4_ori.shape)\n",
    "print(data_alt4_val_ori.shape)\n",
    "\n",
    "model_alt4 = Kmeans(num_cluster = 10)\n",
    "model_alt4.fit(data_alt4_ori, num_alternative=4, max_iter = 1000) #max_iter = 1000\n",
    "_, clustered_data_alt4 = model_alt4.predict_and_cluster(data_alt4_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e17e471",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "delta3_RE = []\n",
    "delta4_RE = []\n",
    "for i in range(len(data_alt3_ori)):\n",
    "    delta3_set = []\n",
    "    delta4_set = []\n",
    "    for j in range(len(data_alt3_val_ori)):\n",
    "        delta3 = np.sum(np.square(data_alt3_ori[i] - data_alt3_val_ori[j]))\n",
    "        delta4 = np.sum(np.square(data_alt4_ori[i] - data_alt4_val_ori[j]))        \n",
    "        delta3_set.append(delta3)\n",
    "        delta4_set.append(delta4)\n",
    "    delta3_RE.append(min(delta3_set))\n",
    "    delta4_RE.append(min(delta4_set))\n",
    "    \n",
    "MREvalue_val_alt3 = np.mean(delta3_RE) # train and val \n",
    "MREvalue_val_alt4 = np.mean(delta4_RE) # trani and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdb6406c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(5, 5))\n",
    "# ax.cla()\n",
    "# ax.imshow(np.reshape(data_alt3[2], (data_alt3.shape[1], data_alt3.shape[2])), cmap='gray')\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(5, 5))\n",
    "# ax.cla()\n",
    "# ax.imshow(np.reshape(data_alt4[99], (data_alt3.shape[1], data_alt3.shape[2])), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6034b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "np.random.shuffle(data_alt_3_4)\n",
    "\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(label_alt_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ef3b284",
   "metadata": {},
   "outputs": [],
   "source": [
    "class election_GAN(object):\n",
    "    def __init__(self,\n",
    "                num_samples = None,\n",
    "                dim_height = None,\n",
    "                dim_width = None,\n",
    "                dim_z = None,\n",
    "                num_class = None,\n",
    "                batch_size = None,\n",
    "                _reuse = None,\n",
    "                MemorizationPenalty_start = None,\n",
    "                ):\n",
    "        \n",
    "        # Definition Params:\n",
    "        self.num_samples = num_samples  \n",
    "        self.dim_height = dim_height    \n",
    "        self.dim_width = dim_width  \n",
    "        self.dim_z = dim_z    \n",
    "        self.num_class = num_class      \n",
    "        self.batch_size = batch_size \n",
    "        self.MemorizationPenalty_start = MemorizationPenalty_start\n",
    "\n",
    "        # Define Network Input:\n",
    "        self.X = tf.placeholder(tf.float32, shape=(None, self.dim_height, self.dim_width, 1))\n",
    "        self.Z = tf.placeholder(tf.float32, shape=(None, 1, 1, self.dim_z))\n",
    "        self.Y_label = tf.placeholder(tf.float32, shape=(None, 1, 1, self.num_class))\n",
    "        self.Y_fill = tf.placeholder(tf.float32, shape=(None, self.dim_height, self.dim_width, self.num_class))\n",
    "        self.isTrain = tf.placeholder(dtype=tf.bool)\n",
    "        self.keep_prob_feed = tf.placeholder(tf.float32)\n",
    "        self.global_step = tf.Variable(0, trainable = False)\n",
    "        self.MREvalue_val_alt3 = tf.placeholder(tf.float32)\n",
    "        self.MREvalue_val_alt4 = tf.placeholder(tf.float32)\n",
    "        self.MRE_val_index = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.dat_alt3_ori = tf.placeholder(tf.float32, shape=(None, 6))\n",
    "        self.dat_alt4_ori = tf.placeholder(tf.float32, shape=(None, 24))\n",
    "        self.MemorizationPenalty_number = tf.Variable(0, trainable = False)\n",
    "        self.n_critic = tf.Variable(0, trainable = False)\n",
    "        \n",
    "        # Network:\n",
    "        self._GEN(self.Z, self.Y_label, self.keep_prob_feed, self.isTrain, _reuse)\n",
    "        \n",
    "        self._DIS(self.X, self.Y_fill, self.keep_prob_feed, self.isTrain, _reuse)\n",
    "            \n",
    "        with tf.variable_scope('object_cost_function', reuse=tf.AUTO_REUSE):\n",
    "            self._object_cost_function() \n",
    "        \n",
    "    def _GEN(self, Z, Y_label, keep_prob_feed, isTrain=True, _reuse=False):\n",
    "\n",
    "        with tf.variable_scope(\"GEN\", reuse=_reuse):\n",
    "            dim = 128\n",
    "\n",
    "            # concat layer\n",
    "            Z_ = tf.concat([Z, Y_label], 3)  #(batch_szie, 1, 1, dim_z + num_class)\n",
    "            Z_ = tf.reshape(Z_, (-1, self.dim_z + self.num_class))        \n",
    "\n",
    "            # FCN \n",
    "            hidden = tf.layers.dense(Z_, units = 2*2*3*dim)\n",
    "#             hidden = tf.layers.batch_normalization(hidden, training=isTrain)\n",
    "            hidden = tf.nn.relu(hidden)\n",
    "            hidden = tf.reshape(hidden, (-1, 2, 2, 3*dim)) \n",
    "\n",
    "            # CNN 1 \n",
    "            hidden = tf.layers.conv2d_transpose(hidden, 2*dim, [3, 3], strides=(1, 2), padding='valid')\n",
    "#             hidden = tf.layers.batch_normalization(hidden, training=isTrain)\n",
    "            hidden = tf.nn.relu(hidden)\n",
    "\n",
    "            # CNN 2 \n",
    "            hidden = tf.layers.conv2d_transpose(hidden, 1*dim, [3, 3], strides=(3, 3), padding='same')\n",
    "#             hidden = tf.layers.batch_normalization(hidden, training=isTrain)\n",
    "            hidden = tf.nn.relu(hidden)     \n",
    "\n",
    "            # CNN 3 \n",
    "            hidden = tf.layers.conv2d_transpose(hidden, 1, [3, 3], strides=(2, 2), padding='same')         \n",
    "            output = hidden\n",
    "\n",
    "            return output     \n",
    "    \n",
    "    def _DIS(self, X, Y_fill, keep_prob_feed, isTrain=True, _reuse=False):\n",
    "\n",
    "        with tf.variable_scope(\"DIS\", reuse=_reuse):\n",
    "            dim = 128\n",
    "            leak = 0.2\n",
    "            # concat layer\n",
    "            X_ = tf.concat([X, Y_fill], 3)  #(batch_size, dim_height, dim_width, channel + num_class)\n",
    "\n",
    "            # CNN 1 \n",
    "            hidden = tf.layers.conv2d(X_, 1*dim, [4, 5], strides=(2, 2), padding='same', use_bias=False)\n",
    "            hidden = ((0.5 * (1 + leak)) * hidden) + ((0.5 * (1 - leak)) * tf.abs(hidden)) #lrelu\n",
    "\n",
    "            # CNN 2 \n",
    "            hidden = tf.layers.conv2d(hidden, 2*dim, [4, 5], strides=(2, 2), padding='same', use_bias=False)\n",
    "            hidden = ((0.5 * (1 + leak)) * hidden) + ((0.5 * (1 - leak)) * tf.abs(hidden)) #lrelu\n",
    "\n",
    "            # CNN 3\n",
    "            hidden = tf.layers.conv2d(hidden, 2*dim, [4, 5], strides=(2, 2), padding='same', use_bias=False)\n",
    "            hidden = ((0.5 * (1 + leak)) * hidden) + ((0.5 * (1 - leak)) * tf.abs(hidden)) #lrelu\n",
    "\n",
    "            # Flatten layer\n",
    "            hidden = tf.layers.flatten(hidden)        \n",
    "            score = tf.layers.dense(hidden, units = 1)\n",
    "            score = tf.reshape(score, (-1, 1))          \n",
    "\n",
    "            return score\n",
    "    \n",
    "                        \n",
    "    def _resize_fault3(self, x, img_size = np.math.factorial(6), ori_size = np.math.factorial(3)):  \n",
    "        x = tf.reshape(x, (self.dim_height*self.dim_width,))  \n",
    "        cell_size = int(img_size//ori_size)\n",
    "        count = tf.constant(0) \n",
    "        num_iter = tf.constant(int(x.shape[0] // cell_size))  \n",
    "        \n",
    "        dat = tf.Variable([])\n",
    "        dat_ = tf.zeros([6,])\n",
    "        def cond(count, num_iter, dat):\n",
    "            return  count < num_iter\n",
    "        def body(count, num_iter, dat):\n",
    "            k = x[(count)*cell_size:(count+1)*cell_size]\n",
    "            res = tf.reduce_mean(k)\n",
    "            dat = tf.concat([dat, [res]], 0)\n",
    "    \n",
    "            count = count + 1\n",
    "            return count, num_iter, dat\n",
    "        count, num_iter, dat = tf.while_loop(cond, body, [count, num_iter, dat]\n",
    "        , shape_invariants=[count.get_shape(), num_iter.get_shape(), tf.TensorShape([None,])])      \n",
    "\n",
    "        dat_ = dat + dat_\n",
    "        \n",
    "        return tf.reshape(dat_, (1,6))\n",
    "    \n",
    "    def _resize_fault4(self, x, img_size = np.math.factorial(6), ori_size = np.math.factorial(4)):\n",
    "        x = tf.reshape(x, (self.dim_height*self.dim_width,))  \n",
    "        cell_size = int(img_size//ori_size)\n",
    "        count = tf.constant(0) \n",
    "        num_iter = tf.constant(int(x.shape[0] // cell_size))  \n",
    "        \n",
    "        dat = tf.Variable([])\n",
    "        dat_ = tf.zeros([24,])\n",
    "        def cond(count, num_iter, dat):\n",
    "            return  count < num_iter\n",
    "        def body(count, num_iter, dat):\n",
    "            k = x[(count)*cell_size:(count+1)*cell_size]\n",
    "            res = tf.reduce_mean(k)\n",
    "            dat = tf.concat([dat, [res]], 0)\n",
    "\n",
    "            count = count + 1\n",
    "            return count, num_iter, dat\n",
    "        count, num_iter, dat = tf.while_loop(cond, body, [count, num_iter, dat]\n",
    "        , shape_invariants=[count.get_shape(), num_iter.get_shape(), tf.TensorShape([None,])])         \n",
    "        \n",
    "        dat_ = dat + dat_\n",
    "\n",
    "        return tf.reshape(dat_, (1,24))    \n",
    "    \n",
    "    def _cal_MRE_and_delta_sum1(self, x):\n",
    "        \n",
    "        with tf.variable_scope(\"cal_MRE_and_delta_sum1\", reuse=False):\n",
    "            self.fake_output_mre = tf.reshape(x, (-1, self.dim_height*self.dim_width)) \n",
    "\n",
    "            ''' pick alt3 '''\n",
    "            count = tf.Variable(0, name='count3') \n",
    "            num_iter = tf.constant(self.batch_size, name='num_iter3') \n",
    "            resize_gen = tf.zeros([1,6], name='resize_gen3')\n",
    "            def cond_1(count, num_iter, resize_gen):\n",
    "                return count < num_iter\n",
    "            def body_1(count, num_iter, resize_gen):\n",
    "                resize3 = lambda: self._resize_fault3(self.fake_output_mre[count])\n",
    "                resize4 = lambda: 100 * tf.constant([1]*6, shape=[1,6], dtype=tf.float32)\n",
    "                resize_result = tf.cond(tf.reduce_mean(self.MRE_val_index[count]) > 0, resize4, resize3)\n",
    "                resize_gen = tf.concat([resize_gen, resize_result], 0)\n",
    "\n",
    "                count = count + 1\n",
    "                return count, num_iter, resize_gen\n",
    "            count, num_iter, resize_gen = tf.while_loop(cond_1, body_1, [count, num_iter, resize_gen]\n",
    "                , shape_invariants=[count.get_shape(), num_iter.get_shape(), tf.TensorShape([None, None])])     \n",
    "            resize_gen_alt3 = resize_gen[1:, :]\n",
    "\n",
    "\n",
    "            ''' pick alt4 '''\n",
    "            count = tf.constant(0) \n",
    "            num_iter = tf.constant(self.batch_size)  \n",
    "            resize_gen = tf.zeros([1,24])\n",
    "            def cond_2(count, num_iter, resize_gen):\n",
    "                return  count < num_iter\n",
    "            def body_2(count, num_iter, resize_gen):\n",
    "                resize3 = lambda: 100 * tf.constant([1]*24, shape=[1,24], dtype=tf.float32)\n",
    "                resize4 = lambda: self._resize_fault4(self.fake_output_mre[count])\n",
    "                resize_result = tf.cond(tf.reduce_mean(self.MRE_val_index[count]) > 0, resize4, resize3)\n",
    "                resize_gen = tf.concat([resize_gen, resize_result], 0)\n",
    "\n",
    "                count = count + 1\n",
    "                return count, num_iter, resize_gen\n",
    "            count, num_iter, resize_gen = tf.while_loop(cond_2, body_2, [count, num_iter, resize_gen]\n",
    "                , shape_invariants=[count.get_shape(), num_iter.get_shape(), tf.TensorShape([None, None])])     \n",
    "            resize_gen_alt4 = resize_gen[1:, :]\n",
    "\n",
    "            ''' calculate MRE '''\n",
    "            count_out = tf.constant(0) \n",
    "#             num_iter_out = tf.constant(int(self.num_samples//2))   # use how many training data to calculate MRE\n",
    "            num_iter_out = tf.constant(self.batch_size)   # use how many training data to calculate MRE\n",
    "            delta3_RE = tf.Variable([])\n",
    "            delta4_RE = tf.Variable([])\n",
    "\n",
    "            def cond_3(count_out, num_iter_out, delta3_RE, delta4_RE):\n",
    "                return  count_out < num_iter_out\n",
    "            def body_3(count_out, num_iter_out, delta3_RE, delta4_RE):\n",
    "\n",
    "                delta3_set = tf.Variable([])\n",
    "                delta4_set = tf.Variable([])\n",
    "\n",
    "                count_in = tf.constant(0) \n",
    "                num_iter_in = tf.constant(self.batch_size)              \n",
    "                def cond_4(count_in, num_iter_in, delta3_set, delta4_set):\n",
    "                    return  count_in < num_iter_in\n",
    "                def body_4(count_in, num_iter_in, delta3_set, delta4_set):\n",
    "                    delta3 = tf.reduce_sum(tf.square(tf.subtract(self.dat_alt3_ori[count_out], resize_gen_alt3[count_in])))\n",
    "                    delta4 = tf.reduce_sum(tf.square(tf.subtract(self.dat_alt4_ori[count_out], resize_gen_alt4[count_in])))\n",
    "                    delta3_set = tf.concat([delta3_set, [delta3]], 0)\n",
    "                    delta4_set = tf.concat([delta4_set, [delta4]], 0)\n",
    "\n",
    "                    count_in = count_in + 1\n",
    "                    return count_in, num_iter_in, delta3_set, delta4_set\n",
    "                count_in, num_iter_in, delta3_set, delta4_set = tf.while_loop(cond_4, body_4, \n",
    "                    [count_in, num_iter_in, delta3_set, delta4_set]\n",
    "                    , shape_invariants = [count_in.get_shape(), num_iter_in.get_shape(),\n",
    "                                          tf.TensorShape([None,]), tf.TensorShape([None,]) ]) \n",
    "\n",
    "                delta3_RE = tf.concat([delta3_RE, [tf.reduce_min(tf.stack(delta3_set))]], 0)\n",
    "                delta4_RE = tf.concat([delta4_RE, [tf.reduce_min(tf.stack(delta4_set))]], 0)            \n",
    "\n",
    "                count_out = count_out + 1\n",
    "                return count_out, num_iter_out, delta3_RE, delta4_RE\n",
    "            count_out, num_iter_out, delta3_RE, delta4_RE = tf.while_loop(cond_3, body_3, \n",
    "                [count_out, num_iter_out, delta3_RE, delta4_RE]\n",
    "                , shape_invariants=[count_out.get_shape(), num_iter_out.get_shape()\n",
    "                                    , tf.TensorShape([None,]), tf.TensorShape([None,]) ])     \n",
    "\n",
    "            MRE3 = tf.reduce_mean(delta3_RE)\n",
    "            MRE4 = tf.reduce_mean(delta4_RE)\n",
    "\n",
    "            MRE3_norm = tf.div(tf.abs(tf.subtract(self.MREvalue_val_alt3, MRE3)), self.MREvalue_val_alt3)\n",
    "            MRE4_norm = tf.div(tf.abs(tf.subtract(self.MREvalue_val_alt4, MRE4)), self.MREvalue_val_alt4)             \n",
    "\n",
    "            '''restrict all ranking sum to 1'''\n",
    "            with tf.variable_scope(\"sum1\", reuse=False):\n",
    "                count_sum1 = tf.constant(0) \n",
    "                num_iter_sum1 = tf.constant(self.batch_size)  \n",
    "                \n",
    "                resize_gen_sum1 = tf.Variable([])\n",
    "                def cond_sum1(count_sum1, num_iter_sum1, resize_gen_sum1):\n",
    "                    return count_sum1 < num_iter_sum1\n",
    "                def body_sum1(count_sum1, num_iter_sum1, resize_gen_sum1):\n",
    "                    resize3 = lambda: tf.reduce_sum(self._resize_fault3(self.fake_output_mre[count_sum1]))\n",
    "                    resize4 = lambda: tf.reduce_sum(self._resize_fault4(self.fake_output_mre[count_sum1]))\n",
    "                    resize_result = tf.cond(tf.reduce_mean(self.MRE_val_index[count_sum1]) > 0, resize4, resize3)\n",
    "                    resize_gen_sum1 = tf.concat([resize_gen_sum1, [resize_result]], 0)\n",
    "\n",
    "                    count_sum1 = count_sum1 + 1\n",
    "                    return count_sum1, num_iter_sum1, resize_gen_sum1\n",
    "                count_sum1, num_iter_sum1, resize_gen_sum1 = tf.while_loop(cond_sum1, body_sum1,\n",
    "                    [count_sum1, num_iter_sum1, resize_gen_sum1]\n",
    "                    , shape_invariants=[count_sum1.get_shape(), num_iter_sum1.get_shape(), tf.TensorShape([None,])])     \n",
    "\n",
    "                sum1 = tf.constant([1]*self.batch_size, shape=[self.batch_size,], dtype=tf.float32)\n",
    "                rank_delta_sum1 = tf.reduce_sum(tf.abs(tf.subtract(resize_gen_sum1, sum1)))        \n",
    "\n",
    "                rank_delta_sum1_ = tf.zeros([1,])\n",
    "                rank_delta_sum1_ = rank_delta_sum1_ + rank_delta_sum1\n",
    "                \n",
    "        return MRE3_norm, MRE4_norm, tf.reduce_mean(rank_delta_sum1_)\n",
    "\n",
    "        \n",
    "    def _object_cost_function(self):\n",
    "        # networks : generator\n",
    "        self.fake_output = self._GEN(self.Z, self.Y_label, self.keep_prob_feed, self.isTrain, _reuse=False)\n",
    "\n",
    "        # networks : discriminator\n",
    "        real_score = self._DIS(self.X, self.Y_fill, self.keep_prob_feed, self.isTrain, _reuse=False)\n",
    "        fake_score = self._DIS(self.fake_output, self.Y_fill, self.keep_prob_feed, self.isTrain, _reuse=True)\n",
    "\n",
    "        # Mean Recovery Error (MRE): calculate MRE after epoch is over 2000\n",
    "        self.dat_alt3_ori = tf.cast(self.dat_alt3_ori, dtype=tf.float32)\n",
    "        self.dat_alt4_ori = tf.cast(self.dat_alt4_ori, dtype=tf.float32)\n",
    "        self.fake_output = tf.cast(self.fake_output, dtype=tf.float32)\n",
    "        self.MREvalue_val_alt3 = tf.cast(self.MREvalue_val_alt3, dtype=tf.float32)\n",
    "        self.MREvalue_val_alt4 = tf.cast(self.MREvalue_val_alt4, dtype=tf.float32)\n",
    "        self.MRE_val_index = tf.cast(self.MRE_val_index, dtype=tf.float32)\n",
    "                \n",
    "        def f1(): return self._cal_MRE_and_delta_sum1(self.fake_output)\n",
    "        def f2(): return tf.cast(0, dtype=tf.float32), tf.cast(0, dtype=tf.float32), tf.cast(0, dtype=tf.float32)\n",
    "        self.MRE3_norm, self.MRE4_norm, self.rank_delta_sum1 = tf.cond(\n",
    "            tf.greater(self.global_step, self.MemorizationPenalty_start), f1, f2)\n",
    "        \n",
    "        '''define the loss ops'''\n",
    "        self.D_loss = tf.reduce_mean(-real_score + fake_score)\n",
    "#         self.G_loss = tf.reduce_mean(-fake_score) + 0.5*(self.MRE3_norm + self.MRE4_norm) + 0.5*self.rank_delta_sum1\n",
    "                \n",
    "        def f3(): return tf.reduce_mean(-fake_score)\n",
    "        def f4(): return tf.reduce_mean(-fake_score) + 0.5*(self.MRE3_norm + self.MRE4_norm) + 0.5*self.rank_delta_sum1\n",
    "        self.G_loss = tf.cond(tf.mod(self.n_critic, self.MemorizationPenalty_number) > 0, f3, f4)        \n",
    "        \n",
    "            \n",
    "        ################  Gradient penalty  ################\n",
    "        LAMBDA = 10\n",
    "        alpha = tf.random_uniform(shape=[self.batch_size, 1], minval=0.,maxval=1.)  \n",
    "\n",
    "        real_data = tf.reshape(self.X, (self.batch_size, self.dim_height*self.dim_width))    \n",
    "        fake_data = tf.reshape(self.fake_output, (self.batch_size, self.dim_height*self.dim_width))    \n",
    "\n",
    "        interpolates = (alpha * real_data + ((1 - alpha) * fake_data))    \n",
    "        interpolates_d = tf.reshape(interpolates, (self.batch_size, self.dim_height, self.dim_width, 1))  \n",
    "        interpolates_d = self._DIS(interpolates_d, self.Y_fill, self.keep_prob_feed, self.isTrain, _reuse=True)                   \n",
    "\n",
    "        gradients = tf.gradients(interpolates_d, [interpolates])[0]   \n",
    "        slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))  \n",
    "        gradient_penalty = tf.reduce_mean((slopes-1.)**2) \n",
    "        self.D_loss = self.D_loss + LAMBDA*gradient_penalty\n",
    "        ###############  Gradient penalty  ################\n",
    "\n",
    "\n",
    "        # define the optimizer ops\n",
    "        self.T_vars = tf.trainable_variables()\n",
    "#         self.D_vars = [var for var in self.T_vars if var.name.startswith('DIS')]\n",
    "#         self.G_vars = [var for var in self.T_vars if var.name.startswith('GEN')]\n",
    "        self.D_vars = [var for var in self.T_vars if 'DIS' in var.name]\n",
    "        self.G_vars = [var for var in self.T_vars if 'DIS' not in var.name]\n",
    "                \n",
    "        learning_rate = 0.0001\n",
    "        # define the update ops to run batch normalization\n",
    "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "#             self.D_optim = tf.train.RMSPropOptimizer(learning_rate).minimize(self.D_loss, var_list=self.D_vars)\n",
    "#             self.G_optim = tf.train.RMSPropOptimizer(learning_rate).minimize(self.G_loss, var_list=self.G_vars)\n",
    "            self.D_optim = tf.train.AdamOptimizer(learning_rate, beta1=0.5, beta2=0.9).minimize(self.D_loss, var_list=self.D_vars)\n",
    "            self.G_optim = tf.train.AdamOptimizer(learning_rate, beta1=0.5, beta2=0.9).minimize(self.G_loss, var_list=self.G_vars)\n",
    "            \n",
    "            \n",
    "    def train_model(self,\n",
    "                    x_train = None,     \n",
    "                    y_train = None,     \n",
    "                    keep_prob = None,\n",
    "                    train_epoch = None, \n",
    "                    n_critic = None,\n",
    "                    step_valid = 50,\n",
    "                    step_save_data = 1000,\n",
    "                    iteration_generator = 50,\n",
    "                    MREvalue_val_alt3 = None,\n",
    "                    MREvalue_val_alt4 = None,\n",
    "                    clustered_data_alt3 = None,\n",
    "                    clustered_data_alt4 = None,\n",
    "                    MemorizationPenalty_number = None,\n",
    "                   ): \n",
    "        \n",
    "        self.train_hist = {}\n",
    "        self.train_hist['D_losses'] = []\n",
    "        self.train_hist['G_losses'] = []\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "#         config = tf.ConfigProto(gpu_options = tf.GPUOptions(allow_growth=True))\n",
    "\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = 0.1 # use 90% memory of GPU\n",
    "\n",
    "        with tf.Session(config = config) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print('Optimization start!')\n",
    "            \n",
    "            for epoch in range(train_epoch):\n",
    "                G_losses = []\n",
    "                D_losses = []\n",
    "                \n",
    "                time_start_epoch = time.time()\n",
    "                \n",
    "                for i in range(len(x_train) // self.batch_size): # num_samples / batch_size\n",
    "                    '''#############        Discriminator       #######################'''\n",
    "                    x_ = x_train[i*self.batch_size: (i+1)*self.batch_size] \n",
    "                    x_ = x_.reshape((self.batch_size, self.dim_height, self.dim_width, 1))   #(batch_size, dim_height, dim_width, 1)\n",
    "                    y_label_ = y_train[i*self.batch_size:(i+1)*self.batch_size].reshape([self.batch_size, 1, 1, self.num_class])\n",
    "                    y_fill_ = y_label_ * np.ones([self.batch_size, self.dim_height, self.dim_width, self.num_class]) #(batch_szie, height, width, num_class)\n",
    "                    z_ = np.random.normal(0, 1, (self.batch_size, 1, 1, self.dim_z))\n",
    "\n",
    "                    loss_d, _ = sess.run([self.D_loss, self.D_optim], \n",
    "                                          feed_dict={self.X: x_, \n",
    "                                                     self.Z: z_, \n",
    "                                                     self.Y_fill: y_fill_, \n",
    "                                                     self.Y_label: y_label_, \n",
    "                                                     self.keep_prob_feed: keep_prob,\n",
    "                                                     self.isTrain: True,\n",
    "                                                     self.global_step: epoch})\n",
    "                    D_losses.append(loss_d)\n",
    "\n",
    "                    '''#############        Generator          #######################'''\n",
    "                    if (i+1) % n_critic == 0:   # Train the generator every n_critic iterations\n",
    "                        z_ = np.random.normal(0, 1, (self.batch_size, 1, 1, self.dim_z))\n",
    "                        y_ = []\n",
    "                        for _ in range(self.batch_size): y_.append(random.randrange(0, self.num_class, self.num_class-1))  \n",
    "                        y_ = np.array(y_).reshape([self.batch_size, 1])\n",
    "        #                 y_ = np.random.randint(0, 2, (batch_size, 1))\n",
    "                        y_label_ = one_hot(y_, self.num_class).reshape([self.batch_size, 1, 1, self.num_class])\n",
    "                        y_fill_ = y_label_ * np.ones([self.batch_size, self.dim_height, self.dim_width, self.num_class])\n",
    "                        \n",
    "                        dat_alt3_ori_KmeansSampling = model_alt3.Kmeans_sampling_fn(clustered_data_alt3, 50)\n",
    "                        dat_alt4_ori_KmeansSampling = model_alt4.Kmeans_sampling_fn(clustered_data_alt4, 50)\n",
    "                        \n",
    "                        loss_g, _, MRE3_norm, MRE4_norm, rank_delta_sum1 = sess.run([\n",
    "                            self.G_loss, self.G_optim, self.MRE3_norm, self.MRE4_norm, self.rank_delta_sum1], \n",
    "                                              feed_dict={self.Z: z_, \n",
    "                                                         self.X: x_, \n",
    "                                                         self.Y_fill: y_fill_, \n",
    "                                                         self.Y_label: y_label_, \n",
    "                                                         self.keep_prob_feed: keep_prob,\n",
    "                                                         self.isTrain: True,\n",
    "                                                         self.MREvalue_val_alt3: MREvalue_val_alt3,\n",
    "                                                         self.MREvalue_val_alt4: MREvalue_val_alt4,\n",
    "                                                         self.MRE_val_index: y_,\n",
    "                                                         self.dat_alt3_ori: dat_alt3_ori_KmeansSampling,\n",
    "                                                         self.dat_alt4_ori: dat_alt4_ori_KmeansSampling,\n",
    "                                                         self.global_step: epoch,\n",
    "                                                         self.n_critic: n_critic,\n",
    "                                                         self.MemorizationPenalty_number: MemorizationPenalty_number,\n",
    "                                                        })\n",
    "                        G_losses.append(loss_g)\n",
    "#                         print('MRE3_norm ', MRE3_norm)\n",
    "#                         print('MRE4_norm ', MRE4_norm)\n",
    "#                         print('rank_delta_sum1 ', rank_delta_sum1)\n",
    "\n",
    "            \n",
    "            ############        print result      #######################\n",
    "                if (epoch+1) % 1 == 0:\n",
    "                    print('[%d/%d] loss_d: %.3f, loss_g: %.3f'%((epoch + 1), train_epoch, np.mean(D_losses), np.mean(G_losses)))\n",
    "                    self.train_hist['D_losses'].append(np.mean(D_losses))\n",
    "                    self.train_hist['G_losses'].append(np.mean(G_losses))\n",
    "\n",
    "            ############        valid      #######################\n",
    "                if (epoch+1) % step_valid == 0:\n",
    "                   ############        num_alternative = 3      #######################\n",
    "                    z_ = np.random.normal(0, 1, (self.batch_size, 1, 1, self.dim_z))   \n",
    "\n",
    "                    y_ = np.zeros([self.batch_size, 1]) + 0\n",
    "                    y_label_ = one_hot(y_, self.num_class).reshape([self.batch_size, 1, 1, self.num_class])\n",
    "                    y_fill_ = y_label_ * np.ones([self.batch_size, self.dim_height, self.dim_width, self.num_class])\n",
    "                    output_g_alt3 = sess.run([self.fake_output], \n",
    "                                          feed_dict={self.Z: z_, \n",
    "                                                     self.X: x_, \n",
    "                                                     self.Y_fill: y_fill_, \n",
    "                                                     self.Y_label: y_label_, \n",
    "                                                     self.keep_prob_feed: 1,\n",
    "                                                     self.isTrain: False})    \n",
    "                    output_g_alt3_ = np.array(output_g_alt3).reshape([self.batch_size, self.dim_height, self.dim_width])[0]\n",
    "\n",
    "                    ############        num_alternative = 4      #######################\n",
    "                    z_ = np.random.normal(0, 1, (self.batch_size, 1, 1, self.dim_z))   \n",
    "\n",
    "                    y_ = np.zeros([self.batch_size, 1]) + 1\n",
    "                    y_label_ = one_hot(y_, self.num_class).reshape([self.batch_size, 1, 1, self.num_class])\n",
    "                    y_fill_ = y_label_ * np.ones([self.batch_size, self.dim_height, self.dim_width, self.num_class])\n",
    "                    output_g_alt4 = sess.run([self.fake_output], \n",
    "                                          feed_dict={self.Z: z_, \n",
    "                                                     self.X: x_, \n",
    "                                                     self.Y_fill: y_fill_, \n",
    "                                                     self.Y_label: y_label_, \n",
    "                                                     self.keep_prob_feed: 1,\n",
    "                                                     self.isTrain: False})  \n",
    "                    output_g_alt4_ = np.array(output_g_alt4).reshape([self.batch_size, self.dim_height, self.dim_width])[0]\n",
    "                                     \n",
    "                    plt.figure(epoch)\n",
    "                    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "                    ax.cla()\n",
    "                    ax.imshow(np.reshape(output_g_alt3_, (self.dim_height, self.dim_width)), cmap='gray')\n",
    "\n",
    "                    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "                    ax.cla()\n",
    "                    ax.imshow(np.reshape(output_g_alt4_, (self.dim_height, self.dim_width)), cmap='gray')            \n",
    "                    plt.show()\n",
    "\n",
    "            ############        save per 1000 epoch      #######################\n",
    "                if (epoch+1) % step_save_data == 0:\n",
    "                    \n",
    "                    generated_3alt = []\n",
    "                    generated_4alt = []                     \n",
    "                    for _ in range(iteration_generator):  \n",
    "                         ############        num_alternative = 3      #######################\n",
    "                        z_ = np.random.normal(0, 1, (self.batch_size, 1, 1, self.dim_z))   \n",
    "                        y_ = np.zeros([self.batch_size, 1]) + 0\n",
    "                        y_label_ = one_hot(y_, self.num_class).reshape([self.batch_size, 1, 1, self.num_class])\n",
    "\n",
    "                        output_g_alt3 = sess.run([self.fake_output], feed_dict={self.Z: z_, self.Y_label: y_label_})    \n",
    "                        generated_3alt.append(np.array(output_g_alt3).reshape([self.batch_size, self.dim_height, self.dim_width]))\n",
    "\n",
    "                        ############        num_alternative = 4      #######################\n",
    "                        z_ = np.random.normal(0, 1, (self.batch_size, 1, 1, self.dim_z))   \n",
    "                        y_ = np.zeros([self.batch_size, 1]) + 1\n",
    "                        y_label_ = one_hot(y_, self.num_class).reshape([self.batch_size, 1, 1, self.num_class])\n",
    "\n",
    "                        output_g_alt4 = sess.run([self.fake_output], feed_dict={self.Z: z_, self.Y_label: y_label_})  \n",
    "                        generated_4alt.append(np.array(output_g_alt4).reshape([self.batch_size, self.dim_height, self.dim_width]))\n",
    "\n",
    "                    generated_3alt = np.array(generated_3alt).reshape([iteration_generator*self.batch_size, self.dim_height, self.dim_width])\n",
    "                  \n",
    "                    gen_alt3_ori = resize_to_ori(generated_3alt, np.math.factorial(6), 30, 24, np.math.factorial(3), self.batch_size, iteration_generator)\n",
    "                    gen_alt3_pd = pd.DataFrame(gen_alt3_ori, columns = ['ABC', 'ACB', 'BAC', 'BCA', 'CAB', 'CBA'])\n",
    "                    gen_alt3_pd.to_csv(generated_path + 'generated_atl3_' + str(epoch) + '.csv')                    \n",
    "                    \n",
    "                    generated_4alt = np.array(generated_4alt).reshape([iteration_generator*self.batch_size, self.dim_height, self.dim_width])     \n",
    "                    \n",
    "                    gen_alt4_ori = resize_to_ori(generated_4alt, np.math.factorial(6), 30, 24, np.math.factorial(4), self.batch_size, iteration_generator)\n",
    "                    gen_alt4_pd = pd.DataFrame(gen_alt4_ori, columns = ['ABCD', 'ACBD', 'BACD', 'BCAD', 'CABD', 'CBAD', 'DABC',\n",
    "                           'DACB', 'DBAC', 'DBCA', 'DCAB', 'DCBA', 'ADBC', 'ADCB', 'BDAC', 'BDCA',\n",
    "                           'CDAB', 'CDBA', 'ABDC', 'ACDB', 'BADC', 'BCDA', 'CADB', 'CBDA'])\n",
    "#                     print(gen_alt4_pd.shape)\n",
    "                    gen_alt4_pd.to_csv(generated_path + 'generated_atl4_' + str(epoch) + '.csv')   \n",
    "                    \n",
    "\n",
    "                time_end_epoch = time.time()\n",
    "                print('Time cost in one epoch', time_end_epoch - time_start_epoch,'s') \n",
    "                \n",
    "            ###########        save      #######################\n",
    "            saver.save(sess, saver_path)   \n",
    "            print('save success')\n",
    "\n",
    "            sess.close()\n",
    "            \n",
    "        print(\"Optimization Finished!\")\n",
    "        \n",
    "    '''loss curve'''\n",
    "    def show_train_hist(self):\n",
    "        x = range(len(self.train_hist['D_losses']))\n",
    "\n",
    "        y1 = self.train_hist['D_losses']\n",
    "        y2 = self.train_hist['G_losses']\n",
    "\n",
    "        plt.plot(x, y1, label='D_loss')\n",
    "        plt.plot(x, y2, label='G_loss')\n",
    "\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "\n",
    "        plt.legend(loc=4)  \n",
    "        plt.grid(True)\n",
    "        plt.tight_layout() \n",
    "        plt.title(\"Training Losses\")\n",
    "\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c99b20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = data_alt_3_4.shape[0]\n",
    "dim_height = data_alt_3_4.shape[1]\n",
    "dim_width = data_alt_3_4.shape[2]\n",
    "dim_z = 128\n",
    "num_class = label_alt_onehot.shape[-1]\n",
    "batch_size = 50\n",
    "MemorizationPenalty_start = 2000 #2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "799334a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = election_GAN(\n",
    "                num_samples = num_samples,\n",
    "                dim_height = dim_height,\n",
    "                dim_width = dim_width,\n",
    "                dim_z = dim_z,\n",
    "                num_class = num_class,\n",
    "                batch_size = batch_size,\n",
    "                MemorizationPenalty_start = MemorizationPenalty_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c660c1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization start!\n",
      "[1/1] loss_d: 1.648, loss_g: 1.223\n",
      "Time cost in one epoch 4.820284843444824 s\n",
      "save success\n",
      "Optimization Finished!\n",
      "Total Time cost 8.176442384719849 s\n"
     ]
    }
   ],
   "source": [
    "time_start=time.time()\n",
    "\n",
    "model.train_model(\n",
    "    x_train = data_alt_3_4,     \n",
    "    y_train = label_alt_onehot,     \n",
    "    keep_prob = 0.5, #0.5\n",
    "    train_epoch = 17000, #17000\n",
    "    n_critic = 5,#5\n",
    "    step_valid = 50, #50\n",
    "    step_save_data = 1000, #1000\n",
    "    iteration_generator = 50,\n",
    "    MREvalue_val_alt3 = MREvalue_val_alt3,\n",
    "    MREvalue_val_alt4 = MREvalue_val_alt4,\n",
    "    clustered_data_alt3 = clustered_data_alt3,\n",
    "    clustered_data_alt4 = clustered_data_alt4,\n",
    "    MemorizationPenalty_number = 1, # penalty (num_samples (2000)//batch_size (50)//MemorizationPenalty_number(10) ) times per epoch\n",
    "    )\n",
    "\n",
    "time_end=time.time()\n",
    "print('Total Time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88ed8ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.show_train_hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828aba7c",
   "metadata": {},
   "source": [
    "# Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81da2d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class election_GAN_GEN(object):\n",
    "    def __init__(self,\n",
    "                dim_height = None,\n",
    "                dim_width = None,\n",
    "                dim_z = None,\n",
    "                num_class = None,\n",
    "                batch_size = None,):\n",
    "        \n",
    "        # Definition Params:\n",
    "        self.dim_height = dim_height    \n",
    "        self.dim_width = dim_width  \n",
    "        self.dim_z = dim_z    \n",
    "        self.num_class = num_class      \n",
    "        self.batch_size = batch_size  \n",
    "\n",
    "        # Define Network Input:\n",
    "        self.Z = tf.placeholder(tf.float32, shape=(None, 1, 1, self.dim_z))\n",
    "        self.Y_label = tf.placeholder(tf.float32, shape=(None, 1, 1, self.num_class))\n",
    "\n",
    "        # Network:\n",
    "        self._GEN(self.Z, self.Y_label)\n",
    "\n",
    "        # get the generated data\n",
    "        with tf.variable_scope('object_cost_function', reuse=tf.AUTO_REUSE):\n",
    "            self.call_GEN()\n",
    "        \n",
    "    def _GEN(self, Z, Y_label):            \n",
    "        keep_prob_feed = 1\n",
    "        isTrain=False \n",
    "        dim = 128\n",
    "        \n",
    "        with tf.variable_scope(\"GEN\", reuse=False):\n",
    "            # concat layer\n",
    "            Z_ = tf.concat([Z, Y_label], 3)  #(batch_szie, 1, 1, dim_z + num_class)\n",
    "            Z_ = tf.reshape(Z_, (-1, self.dim_z + self.num_class))        \n",
    "\n",
    "            # FCN \n",
    "            hidden = tf.layers.dense(Z_, units = 2*2*3*dim)\n",
    "#             hidden = tf.layers.batch_normalization(hidden, training=isTrain)\n",
    "            hidden = tf.nn.relu(hidden)\n",
    "            hidden = tf.reshape(hidden, (-1, 2, 2, 3*dim)) \n",
    "\n",
    "            # CNN 1 \n",
    "            hidden = tf.layers.conv2d_transpose(hidden, 2*dim, [3, 3], strides=(1, 2), padding='valid')\n",
    "#             hidden = tf.layers.batch_normalization(hidden, training=isTrain)\n",
    "            hidden = tf.nn.relu(hidden)\n",
    "\n",
    "            # CNN 2 \n",
    "            hidden = tf.layers.conv2d_transpose(hidden, 1*dim, [3, 3], strides=(3, 3), padding='same')\n",
    "#             hidden = tf.layers.batch_normalization(hidden, training=isTrain)\n",
    "            hidden = tf.nn.relu(hidden)     \n",
    "\n",
    "            # CNN 3 \n",
    "            hidden = tf.layers.conv2d_transpose(hidden, 1, [3, 3], strides=(2, 2), padding='same')         \n",
    "            output = hidden\n",
    "\n",
    "            return output     \n",
    "    \n",
    "    def call_GEN(self):\n",
    "        self.fake_output = self._GEN(self.Z, self.Y_label)\n",
    "\n",
    "    def GEN_model(self, path, iteration): \n",
    "    \n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "            saver.restore(sess, tf.train.latest_checkpoint(path))      \n",
    "            \n",
    "            generated_3alt = []\n",
    "            generated_4alt = []  \n",
    "            \n",
    "            for _ in range(iteration): \n",
    "                '''#############        Generator          #######################'''\n",
    "                ############        num_alternative = 3      #######################\n",
    "                z_ = np.random.normal(0, 1, (self.batch_size, 1, 1, self.dim_z))   \n",
    "                y_ = np.zeros([self.batch_size, 1]) + 0\n",
    "                y_label_ = one_hot(y_, self.num_class).reshape([self.batch_size, 1, 1, self.num_class])\n",
    "\n",
    "                output_g_alt3 = sess.run([self.fake_output], feed_dict={self.Z: z_, self.Y_label: y_label_})    \n",
    "                output_g_alt3_ = np.array(output_g_alt3).reshape([self.batch_size, self.dim_height, self.dim_width])\n",
    "                generated_3alt.append(output_g_alt3_)\n",
    "                \n",
    "                ############        num_alternative = 4      #######################\n",
    "                z_ = np.random.normal(0, 1, (self.batch_size, 1, 1, self.dim_z))   \n",
    "                y_ = np.zeros([self.batch_size, 1]) + 1\n",
    "                y_label_ = one_hot(y_, self.num_class).reshape([self.batch_size, 1, 1, self.num_class])\n",
    "\n",
    "                output_g_alt4 = sess.run([self.fake_output], feed_dict={self.Z: z_, self.Y_label: y_label_})  \n",
    "                output_g_alt4_ = np.array(output_g_alt4).reshape([self.batch_size, self.dim_height, self.dim_width])\n",
    "                generated_4alt.append(output_g_alt4_)\n",
    "\n",
    "            generated_3alt = np.array(generated_3alt).reshape([iteration*self.batch_size, self.dim_height, self.dim_width])\n",
    "            generated_4alt = np.array(generated_4alt).reshape([iteration*self.batch_size, self.dim_height, self.dim_width])\n",
    "                \n",
    "            return generated_3alt, generated_4alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e87163b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c002f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = election_GAN_GEN(\n",
    "                dim_height = dim_height,\n",
    "                dim_width = dim_width,\n",
    "                dim_z = dim_z,\n",
    "                num_class = num_class,\n",
    "                batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f77d2d3",
   "metadata": {},
   "source": [
    "shape[0]: number of generated data\n",
    "\n",
    "shape[1], shape[2]: data size before resize back to original size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50d5d988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from election_GAN/model_.ckpt\n",
      "(2500, 24, 30)\n",
      "(2500, 24, 30)\n"
     ]
    }
   ],
   "source": [
    "iteration_generator = 50\n",
    "gen_alt3, gen_alt4 = generator.GEN_model(path = restore_path, \n",
    "                                         iteration = iteration_generator) #iteration: how many times generator is used?\n",
    "print(gen_alt3.shape) \n",
    "print(gen_alt4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b12273be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.cla()\n",
    "ax.imshow(np.reshape(gen_alt3[0], (dim_height, dim_width)), cmap='gray')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.cla()\n",
    "ax.imshow(np.reshape(gen_alt4[0], (dim_height, dim_width)), cmap='gray')            \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e973dce",
   "metadata": {},
   "source": [
    "# Resize to original shpe & Save as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d84d24d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ABC</th>\n",
       "      <th>ACB</th>\n",
       "      <th>BAC</th>\n",
       "      <th>BCA</th>\n",
       "      <th>CAB</th>\n",
       "      <th>CBA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.031143</td>\n",
       "      <td>0.036444</td>\n",
       "      <td>0.060429</td>\n",
       "      <td>0.052578</td>\n",
       "      <td>0.043112</td>\n",
       "      <td>0.034809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.027901</td>\n",
       "      <td>0.033042</td>\n",
       "      <td>0.064853</td>\n",
       "      <td>0.054499</td>\n",
       "      <td>0.048050</td>\n",
       "      <td>0.038698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.031035</td>\n",
       "      <td>0.037427</td>\n",
       "      <td>0.067127</td>\n",
       "      <td>0.058191</td>\n",
       "      <td>0.047085</td>\n",
       "      <td>0.039110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.028585</td>\n",
       "      <td>0.034729</td>\n",
       "      <td>0.062424</td>\n",
       "      <td>0.056420</td>\n",
       "      <td>0.045965</td>\n",
       "      <td>0.037768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.025668</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.058562</td>\n",
       "      <td>0.051017</td>\n",
       "      <td>0.038192</td>\n",
       "      <td>0.032806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ABC       ACB       BAC       BCA       CAB       CBA\n",
       "0  0.031143  0.036444  0.060429  0.052578  0.043112  0.034809\n",
       "1  0.027901  0.033042  0.064853  0.054499  0.048050  0.038698\n",
       "2  0.031035  0.037427  0.067127  0.058191  0.047085  0.039110\n",
       "3  0.028585  0.034729  0.062424  0.056420  0.045965  0.037768\n",
       "4  0.025668  0.030103  0.058562  0.051017  0.038192  0.032806"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_size = np.math.factorial(6)\n",
    "img_width = 30\n",
    "img_height = 24\n",
    "ori_size = np.math.factorial(3)\n",
    "\n",
    "gen_alt3_ori = resize_to_ori(gen_alt3, img_size, img_width, img_height, ori_size, batch_size, iteration_generator)\n",
    "gen_alt3_pd = pd.DataFrame(gen_alt3_ori, columns = ['ABC', 'ACB', 'BAC', 'BCA', 'CAB', 'CBA'])\n",
    "print(gen_alt3_pd.shape)\n",
    "gen_alt3_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "266b6adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_alt3_pd.to_csv(generated_path + 'generated_atl3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f74596b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 24)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ABCD</th>\n",
       "      <th>ACBD</th>\n",
       "      <th>BACD</th>\n",
       "      <th>BCAD</th>\n",
       "      <th>CABD</th>\n",
       "      <th>CBAD</th>\n",
       "      <th>DABC</th>\n",
       "      <th>DACB</th>\n",
       "      <th>DBAC</th>\n",
       "      <th>DBCA</th>\n",
       "      <th>...</th>\n",
       "      <th>BDAC</th>\n",
       "      <th>BDCA</th>\n",
       "      <th>CDAB</th>\n",
       "      <th>CDBA</th>\n",
       "      <th>ABDC</th>\n",
       "      <th>ACDB</th>\n",
       "      <th>BADC</th>\n",
       "      <th>BCDA</th>\n",
       "      <th>CADB</th>\n",
       "      <th>CBDA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010085</td>\n",
       "      <td>0.016169</td>\n",
       "      <td>0.051681</td>\n",
       "      <td>0.027459</td>\n",
       "      <td>0.035688</td>\n",
       "      <td>0.022404</td>\n",
       "      <td>0.034177</td>\n",
       "      <td>0.030040</td>\n",
       "      <td>0.098269</td>\n",
       "      <td>0.046347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082614</td>\n",
       "      <td>0.038212</td>\n",
       "      <td>0.064468</td>\n",
       "      <td>0.036536</td>\n",
       "      <td>0.044684</td>\n",
       "      <td>0.018856</td>\n",
       "      <td>0.047974</td>\n",
       "      <td>0.020969</td>\n",
       "      <td>0.035647</td>\n",
       "      <td>0.025769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010665</td>\n",
       "      <td>0.016530</td>\n",
       "      <td>0.049857</td>\n",
       "      <td>0.026092</td>\n",
       "      <td>0.035404</td>\n",
       "      <td>0.025602</td>\n",
       "      <td>0.033698</td>\n",
       "      <td>0.029202</td>\n",
       "      <td>0.083720</td>\n",
       "      <td>0.043973</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083783</td>\n",
       "      <td>0.036438</td>\n",
       "      <td>0.065871</td>\n",
       "      <td>0.042921</td>\n",
       "      <td>0.044769</td>\n",
       "      <td>0.016531</td>\n",
       "      <td>0.048769</td>\n",
       "      <td>0.024633</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>0.026986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013154</td>\n",
       "      <td>0.016182</td>\n",
       "      <td>0.054869</td>\n",
       "      <td>0.028276</td>\n",
       "      <td>0.040836</td>\n",
       "      <td>0.025757</td>\n",
       "      <td>0.034853</td>\n",
       "      <td>0.030822</td>\n",
       "      <td>0.091022</td>\n",
       "      <td>0.045085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.039960</td>\n",
       "      <td>0.064638</td>\n",
       "      <td>0.043647</td>\n",
       "      <td>0.041955</td>\n",
       "      <td>0.018753</td>\n",
       "      <td>0.047041</td>\n",
       "      <td>0.022712</td>\n",
       "      <td>0.039953</td>\n",
       "      <td>0.024840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010569</td>\n",
       "      <td>0.015322</td>\n",
       "      <td>0.049587</td>\n",
       "      <td>0.027638</td>\n",
       "      <td>0.039598</td>\n",
       "      <td>0.023585</td>\n",
       "      <td>0.035269</td>\n",
       "      <td>0.026505</td>\n",
       "      <td>0.092555</td>\n",
       "      <td>0.040831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086712</td>\n",
       "      <td>0.039096</td>\n",
       "      <td>0.062077</td>\n",
       "      <td>0.043668</td>\n",
       "      <td>0.043147</td>\n",
       "      <td>0.020618</td>\n",
       "      <td>0.057171</td>\n",
       "      <td>0.022332</td>\n",
       "      <td>0.046662</td>\n",
       "      <td>0.024958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010238</td>\n",
       "      <td>0.016221</td>\n",
       "      <td>0.051417</td>\n",
       "      <td>0.027992</td>\n",
       "      <td>0.035828</td>\n",
       "      <td>0.026314</td>\n",
       "      <td>0.035650</td>\n",
       "      <td>0.025082</td>\n",
       "      <td>0.084703</td>\n",
       "      <td>0.044948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085374</td>\n",
       "      <td>0.036027</td>\n",
       "      <td>0.071652</td>\n",
       "      <td>0.048289</td>\n",
       "      <td>0.040744</td>\n",
       "      <td>0.021245</td>\n",
       "      <td>0.051021</td>\n",
       "      <td>0.022441</td>\n",
       "      <td>0.044159</td>\n",
       "      <td>0.026253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ABCD      ACBD      BACD      BCAD      CABD      CBAD      DABC  \\\n",
       "0  0.010085  0.016169  0.051681  0.027459  0.035688  0.022404  0.034177   \n",
       "1  0.010665  0.016530  0.049857  0.026092  0.035404  0.025602  0.033698   \n",
       "2  0.013154  0.016182  0.054869  0.028276  0.040836  0.025757  0.034853   \n",
       "3  0.010569  0.015322  0.049587  0.027638  0.039598  0.023585  0.035269   \n",
       "4  0.010238  0.016221  0.051417  0.027992  0.035828  0.026314  0.035650   \n",
       "\n",
       "       DACB      DBAC      DBCA  ...      BDAC      BDCA      CDAB      CDBA  \\\n",
       "0  0.030040  0.098269  0.046347  ...  0.082614  0.038212  0.064468  0.036536   \n",
       "1  0.029202  0.083720  0.043973  ...  0.083783  0.036438  0.065871  0.042921   \n",
       "2  0.030822  0.091022  0.045085  ...  0.087410  0.039960  0.064638  0.043647   \n",
       "3  0.026505  0.092555  0.040831  ...  0.086712  0.039096  0.062077  0.043668   \n",
       "4  0.025082  0.084703  0.044948  ...  0.085374  0.036027  0.071652  0.048289   \n",
       "\n",
       "       ABDC      ACDB      BADC      BCDA      CADB      CBDA  \n",
       "0  0.044684  0.018856  0.047974  0.020969  0.035647  0.025769  \n",
       "1  0.044769  0.016531  0.048769  0.024633  0.038000  0.026986  \n",
       "2  0.041955  0.018753  0.047041  0.022712  0.039953  0.024840  \n",
       "3  0.043147  0.020618  0.057171  0.022332  0.046662  0.024958  \n",
       "4  0.040744  0.021245  0.051021  0.022441  0.044159  0.026253  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_size = np.math.factorial(6)\n",
    "img_width = 30\n",
    "img_height = 24\n",
    "ori_size = np.math.factorial(4)\n",
    "\n",
    "gen_alt4_ori = resize_to_ori(gen_alt4, img_size, img_width, img_height, ori_size, batch_size, iteration_generator)\n",
    "gen_alt4_pd = pd.DataFrame(gen_alt4_ori, columns = ['ABCD', 'ACBD', 'BACD', 'BCAD', 'CABD', 'CBAD', 'DABC',\n",
    "       'DACB', 'DBAC', 'DBCA', 'DCAB', 'DCBA', 'ADBC', 'ADCB', 'BDAC', 'BDCA',\n",
    "       'CDAB', 'CDBA', 'ABDC', 'ACDB', 'BADC', 'BCDA', 'CADB', 'CBDA'])\n",
    "print(gen_alt4_pd.shape)\n",
    "gen_alt4_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "feab2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_alt4_pd.to_csv(generated_path + 'generated_atl4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9939b106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995dc923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.12",
   "language": "python",
   "name": "tf1.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
