{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87ed5f26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WIN10\\anaconda3\\envs\\tf1.12\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\WIN10\\anaconda3\\envs\\tf1.12\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\WIN10\\anaconda3\\envs\\tf1.12\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\WIN10\\anaconda3\\envs\\tf1.12\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\WIN10\\anaconda3\\envs\\tf1.12\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\WIN10\\anaconda3\\envs\\tf1.12\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os, time, itertools, imageio, pickle, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from utils import one_hot, resize_to_ori_calMRE, resize_to_ori\n",
    "# from Kmeans_sampling import Kmeans\n",
    "\n",
    "tf.reset_default_graph() \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # select GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aea24567",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'election_GAN/'\n",
    "gen_results = 'generated_results/'\n",
    "\n",
    "model = 'model_'+'.ckpt'\n",
    "\n",
    "if not os.path.isdir(folder):\n",
    "    os.mkdir(folder)\n",
    "    \n",
    "if not os.path.isdir(folder + gen_results):\n",
    "    os.mkdir(folder + gen_results)\n",
    "\n",
    "# save ckpt\n",
    "saver_path = os.path.join(folder, model)\n",
    "\n",
    "# read ckpt\n",
    "restore_path = os.path.join(folder)\n",
    "\n",
    "# save generated data\n",
    "generated_path = os.path.join(folder + gen_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffeddad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 24, 30)\n",
      "(1000, 24, 30)\n",
      "(2000, 24, 30)\n"
     ]
    }
   ],
   "source": [
    "img_width = 30\n",
    "img_height = 24\n",
    "\n",
    "data_alt3_all = pd.read_csv('./data/netflix_data_3alt_resize.csv')\n",
    "data_alt3 = data_alt3_all.iloc[:1000,1:].values\n",
    "data_alt3_val = data_alt3_all.iloc[1000:2000,1:].values\n",
    "\n",
    "data_alt3 = data_alt3.reshape([-1, img_height, img_width])\n",
    "print(data_alt3.shape)\n",
    "\n",
    "##################################\n",
    "data_alt4_all = pd.read_csv('./data/netflix_data_4alt_resize.csv')\n",
    "data_alt4 = data_alt4_all.iloc[:1000,1:].values\n",
    "data_alt4_val = data_alt4_all.iloc[1000:2000,1:].values\n",
    "\n",
    "data_alt4 = data_alt4.reshape([-1, img_height, img_width])\n",
    "print(data_alt4.shape)\n",
    "\n",
    "data_alt_3_4 = np.concatenate( (data_alt3, data_alt4), axis = 0)\n",
    "print(data_alt_3_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f16811f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2)\n"
     ]
    }
   ],
   "source": [
    "label_alt3 = np.zeros([data_alt3.shape[0]])\n",
    "label_alt4 = np.zeros([data_alt4.shape[0]]) + 1\n",
    "label_alt_3_4 = np.concatenate( (label_alt3,label_alt4), axis = 0)\n",
    "\n",
    "label_alt_onehot = one_hot(label_alt_3_4, 1 + 1)   \n",
    "print(label_alt_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4647ae49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 6)\n",
      "(1000, 6)\n"
     ]
    }
   ],
   "source": [
    "img_size = np.math.factorial(6)\n",
    "img_width = 30\n",
    "img_height = 24\n",
    "ori_size = np.math.factorial(3)\n",
    "\n",
    "data_alt3_ori = resize_to_ori_calMRE(data_alt3, img_size, img_width, img_height, ori_size)\n",
    "data_alt3_val_ori = resize_to_ori_calMRE(data_alt3_val, img_size, img_width, img_height, ori_size)\n",
    "print(data_alt3_ori.shape)\n",
    "print(data_alt3_val_ori.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d8135b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 24)\n",
      "(1000, 24)\n"
     ]
    }
   ],
   "source": [
    "img_size = np.math.factorial(6)\n",
    "img_width = 30\n",
    "img_height = 24\n",
    "ori_size = np.math.factorial(4)\n",
    "\n",
    "data_alt4_ori = resize_to_ori_calMRE(data_alt4, img_size, img_width, img_height, ori_size)\n",
    "data_alt4_val_ori = resize_to_ori_calMRE(data_alt4_val, img_size, img_width, img_height, ori_size)\n",
    "print(data_alt4_ori.shape)\n",
    "print(data_alt4_val_ori.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e17e471",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "delta3_RE = []\n",
    "delta4_RE = []\n",
    "for i in range(len(data_alt3_ori)):\n",
    "    delta3_set = []\n",
    "    delta4_set = []\n",
    "    for j in range(len(data_alt3_val_ori)):\n",
    "        delta3 = np.sum(np.square(data_alt3_ori[i] - data_alt3_val_ori[j]))\n",
    "        delta4 = np.sum(np.square(data_alt4_ori[i] - data_alt4_val_ori[j]))        \n",
    "        delta3_set.append(delta3)\n",
    "        delta4_set.append(delta4)\n",
    "    delta3_RE.append(min(delta3_set))\n",
    "    delta4_RE.append(min(delta4_set))\n",
    "    \n",
    "MREvalue_val_alt3 = np.mean(delta3_RE) # train and val \n",
    "MREvalue_val_alt4 = np.mean(delta4_RE) # trani and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdb6406c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(5, 5))\n",
    "# ax.cla()\n",
    "# ax.imshow(np.reshape(data_alt3[2], (data_alt3.shape[1], data_alt3.shape[2])), cmap='gray')\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(5, 5))\n",
    "# ax.cla()\n",
    "# ax.imshow(np.reshape(data_alt4[99], (data_alt3.shape[1], data_alt3.shape[2])), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6034b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "np.random.shuffle(data_alt_3_4)\n",
    "\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(label_alt_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ef3b284",
   "metadata": {},
   "outputs": [],
   "source": [
    "class election_GAN(object):\n",
    "    def __init__(self,\n",
    "                num_samples = None,\n",
    "                dim_height = None,\n",
    "                dim_width = None,\n",
    "                dim_z = None,\n",
    "                num_class = None,\n",
    "                batch_size = None,\n",
    "                _reuse = None,\n",
    "                MemorizationPenalty_start = None,\n",
    "                ):\n",
    "        \n",
    "        # Definition Params:\n",
    "        self.num_samples = num_samples  \n",
    "        self.dim_height = dim_height    \n",
    "        self.dim_width = dim_width  \n",
    "        self.dim_z = dim_z    \n",
    "        self.num_class = num_class      \n",
    "        self.batch_size = batch_size \n",
    "        self.MemorizationPenalty_start = MemorizationPenalty_start\n",
    "\n",
    "        # Define Network Input:\n",
    "        self.X = tf.placeholder(tf.float32, shape=(None, self.dim_height, self.dim_width, 1))\n",
    "        self.Z = tf.placeholder(tf.float32, shape=(None, 1, 1, self.dim_z))\n",
    "        self.Y_label = tf.placeholder(tf.float32, shape=(None, 1, 1, self.num_class))\n",
    "        self.Y_fill = tf.placeholder(tf.float32, shape=(None, self.dim_height, self.dim_width, self.num_class))\n",
    "        self.isTrain = tf.placeholder(dtype=tf.bool)\n",
    "        self.keep_prob_feed = tf.placeholder(tf.float32)\n",
    "        self.global_step = tf.Variable(0, trainable = False)\n",
    "        self.MREvalue_val_alt3 = tf.placeholder(tf.float32)\n",
    "        self.MREvalue_val_alt4 = tf.placeholder(tf.float32)\n",
    "        self.MRE_val_index = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.dat_alt3_ori = tf.placeholder(tf.float32, shape=(None, 6))\n",
    "        self.dat_alt4_ori = tf.placeholder(tf.float32, shape=(None, 24))\n",
    "        self.MemorizationPenalty_number = tf.Variable(0, trainable = False)\n",
    "        self.n_critic = tf.Variable(0, trainable = False)\n",
    "        \n",
    "        # Network:\n",
    "        self._GEN(self.Z, self.Y_label, self.keep_prob_feed, self.isTrain, _reuse)\n",
    "        \n",
    "        self._DIS(self.X, self.Y_fill, self.keep_prob_feed, self.isTrain, _reuse)\n",
    "            \n",
    "        with tf.variable_scope('object_cost_function', reuse=tf.AUTO_REUSE):\n",
    "            self._object_cost_function() \n",
    "        \n",
    "    def _GEN(self, Z, Y_label, keep_prob_feed, isTrain=True, _reuse=False):\n",
    "\n",
    "        with tf.variable_scope(\"GEN\", reuse=_reuse):\n",
    "            dim = 128\n",
    "\n",
    "            # concat layer\n",
    "            Z_ = tf.concat([Z, Y_label], 3)  #(batch_szie, 1, 1, dim_z + num_class)\n",
    "            Z_ = tf.reshape(Z_, (-1, self.dim_z + self.num_class))        \n",
    "\n",
    "            # FCN \n",
    "            hidden = tf.layers.dense(Z_, units = 2*2*3*dim)\n",
    "#             hidden = tf.layers.batch_normalization(hidden, training=isTrain)\n",
    "            hidden = tf.nn.relu(hidden)\n",
    "            hidden = tf.reshape(hidden, (-1, 2, 2, 3*dim)) \n",
    "\n",
    "            # CNN 1 \n",
    "            hidden = tf.layers.conv2d_transpose(hidden, 2*dim, [3, 3], strides=(1, 2), padding='valid')\n",
    "#             hidden = tf.layers.batch_normalization(hidden, training=isTrain)\n",
    "            hidden = tf.nn.relu(hidden)\n",
    "\n",
    "            # CNN 2 \n",
    "            hidden = tf.layers.conv2d_transpose(hidden, 1*dim, [3, 3], strides=(3, 3), padding='same')\n",
    "#             hidden = tf.layers.batch_normalization(hidden, training=isTrain)\n",
    "            hidden = tf.nn.relu(hidden)     \n",
    "\n",
    "            # CNN 3 \n",
    "            hidden = tf.layers.conv2d_transpose(hidden, 1, [3, 3], strides=(2, 2), padding='same')         \n",
    "            output = hidden\n",
    "\n",
    "            return output     \n",
    "    \n",
    "    def _DIS(self, X, Y_fill, keep_prob_feed, isTrain=True, _reuse=False):\n",
    "\n",
    "        with tf.variable_scope(\"DIS\", reuse=_reuse):\n",
    "            dim = 128\n",
    "            leak = 0.2\n",
    "            # concat layer\n",
    "            X_ = tf.concat([X, Y_fill], 3)  #(batch_size, dim_height, dim_width, channel + num_class)\n",
    "\n",
    "            # CNN 1 \n",
    "            hidden = tf.layers.conv2d(X_, 1*dim, [4, 5], strides=(2, 2), padding='same', use_bias=False)\n",
    "            hidden = ((0.5 * (1 + leak)) * hidden) + ((0.5 * (1 - leak)) * tf.abs(hidden)) #lrelu\n",
    "\n",
    "            # CNN 2 \n",
    "            hidden = tf.layers.conv2d(hidden, 2*dim, [4, 5], strides=(2, 2), padding='same', use_bias=False)\n",
    "            hidden = ((0.5 * (1 + leak)) * hidden) + ((0.5 * (1 - leak)) * tf.abs(hidden)) #lrelu\n",
    "\n",
    "            # CNN 3\n",
    "            hidden = tf.layers.conv2d(hidden, 2*dim, [4, 5], strides=(2, 2), padding='same', use_bias=False)\n",
    "            hidden = ((0.5 * (1 + leak)) * hidden) + ((0.5 * (1 - leak)) * tf.abs(hidden)) #lrelu\n",
    "\n",
    "            # Flatten layer\n",
    "            hidden = tf.layers.flatten(hidden)        \n",
    "            score = tf.layers.dense(hidden, units = 1)\n",
    "            score = tf.reshape(score, (-1, 1))          \n",
    "\n",
    "            return score\n",
    "    \n",
    "                        \n",
    "    def _resize_fault3(self, x, img_size = np.math.factorial(6), ori_size = np.math.factorial(3)):  \n",
    "        x = tf.reshape(x, (self.dim_height*self.dim_width,))  \n",
    "        cell_size = int(img_size//ori_size)\n",
    "        count = tf.constant(0) \n",
    "        num_iter = tf.constant(int(x.shape[0] // cell_size))  \n",
    "        \n",
    "        dat = tf.Variable([])\n",
    "        dat_ = tf.zeros([6,])\n",
    "        def cond(count, num_iter, dat):\n",
    "            return  count < num_iter\n",
    "        def body(count, num_iter, dat):\n",
    "            k = x[(count)*cell_size:(count+1)*cell_size]\n",
    "            res = tf.reduce_mean(k)\n",
    "            dat = tf.concat([dat, [res]], 0)\n",
    "    \n",
    "            count = count + 1\n",
    "            return count, num_iter, dat\n",
    "        count, num_iter, dat = tf.while_loop(cond, body, [count, num_iter, dat]\n",
    "        , shape_invariants=[count.get_shape(), num_iter.get_shape(), tf.TensorShape([None,])])      \n",
    "\n",
    "        dat_ = dat + dat_\n",
    "        \n",
    "        return tf.reshape(dat_, (1,6))\n",
    "    \n",
    "    def _resize_fault4(self, x, img_size = np.math.factorial(6), ori_size = np.math.factorial(4)):\n",
    "        x = tf.reshape(x, (self.dim_height*self.dim_width,))  \n",
    "        cell_size = int(img_size//ori_size)\n",
    "        count = tf.constant(0) \n",
    "        num_iter = tf.constant(int(x.shape[0] // cell_size))  \n",
    "        \n",
    "        dat = tf.Variable([])\n",
    "        dat_ = tf.zeros([24,])\n",
    "        def cond(count, num_iter, dat):\n",
    "            return  count < num_iter\n",
    "        def body(count, num_iter, dat):\n",
    "            k = x[(count)*cell_size:(count+1)*cell_size]\n",
    "            res = tf.reduce_mean(k)\n",
    "            dat = tf.concat([dat, [res]], 0)\n",
    "\n",
    "            count = count + 1\n",
    "            return count, num_iter, dat\n",
    "        count, num_iter, dat = tf.while_loop(cond, body, [count, num_iter, dat]\n",
    "        , shape_invariants=[count.get_shape(), num_iter.get_shape(), tf.TensorShape([None,])])         \n",
    "        \n",
    "        dat_ = dat + dat_\n",
    "\n",
    "        return tf.reshape(dat_, (1,24))    \n",
    "    \n",
    "    def _cal_MRE_and_delta_sum1(self, x):\n",
    "        \n",
    "        with tf.variable_scope(\"cal_MRE_and_delta_sum1\", reuse=False):\n",
    "            self.fake_output_mre = tf.reshape(x, (-1, self.dim_height*self.dim_width)) \n",
    "\n",
    "            ''' pick alt3 '''\n",
    "            count = tf.Variable(0, name='count3') \n",
    "            num_iter = tf.constant(self.batch_size, name='num_iter3') \n",
    "            resize_gen = tf.zeros([1,6], name='resize_gen3')\n",
    "            def cond_1(count, num_iter, resize_gen):\n",
    "                return count < num_iter\n",
    "            def body_1(count, num_iter, resize_gen):\n",
    "                resize3 = lambda: self._resize_fault3(self.fake_output_mre[count])\n",
    "                resize4 = lambda: 100 * tf.constant([1]*6, shape=[1,6], dtype=tf.float32)\n",
    "                resize_result = tf.cond(tf.reduce_mean(self.MRE_val_index[count]) > 0, resize4, resize3)\n",
    "                resize_gen = tf.concat([resize_gen, resize_result], 0)\n",
    "\n",
    "                count = count + 1\n",
    "                return count, num_iter, resize_gen\n",
    "            count, num_iter, resize_gen = tf.while_loop(cond_1, body_1, [count, num_iter, resize_gen]\n",
    "                , shape_invariants=[count.get_shape(), num_iter.get_shape(), tf.TensorShape([None, None])])     \n",
    "            resize_gen_alt3 = resize_gen[1:, :]\n",
    "\n",
    "\n",
    "            ''' pick alt4 '''\n",
    "            count = tf.constant(0) \n",
    "            num_iter = tf.constant(self.batch_size)  \n",
    "            resize_gen = tf.zeros([1,24])\n",
    "            def cond_2(count, num_iter, resize_gen):\n",
    "                return  count < num_iter\n",
    "            def body_2(count, num_iter, resize_gen):\n",
    "                resize3 = lambda: 100 * tf.constant([1]*24, shape=[1,24], dtype=tf.float32)\n",
    "                resize4 = lambda: self._resize_fault4(self.fake_output_mre[count])\n",
    "                resize_result = tf.cond(tf.reduce_mean(self.MRE_val_index[count]) > 0, resize4, resize3)\n",
    "                resize_gen = tf.concat([resize_gen, resize_result], 0)\n",
    "\n",
    "                count = count + 1\n",
    "                return count, num_iter, resize_gen\n",
    "            count, num_iter, resize_gen = tf.while_loop(cond_2, body_2, [count, num_iter, resize_gen]\n",
    "                , shape_invariants=[count.get_shape(), num_iter.get_shape(), tf.TensorShape([None, None])])     \n",
    "            resize_gen_alt4 = resize_gen[1:, :]\n",
    "\n",
    "            ''' calculate MRE '''\n",
    "            count_out = tf.constant(0) \n",
    "#             num_iter_out = tf.constant(int(self.num_samples//2))   # use how many training data to calculate MRE\n",
    "            num_iter_out = tf.constant(self.batch_size)   # use how many training data to calculate MRE\n",
    "            delta3_RE = tf.Variable([])\n",
    "            delta4_RE = tf.Variable([])\n",
    "\n",
    "            def cond_3(count_out, num_iter_out, delta3_RE, delta4_RE):\n",
    "                return  count_out < num_iter_out\n",
    "            def body_3(count_out, num_iter_out, delta3_RE, delta4_RE):\n",
    "\n",
    "                delta3_set = tf.Variable([])\n",
    "                delta4_set = tf.Variable([])\n",
    "\n",
    "                count_in = tf.constant(0) \n",
    "                num_iter_in = tf.constant(self.batch_size)              \n",
    "                def cond_4(count_in, num_iter_in, delta3_set, delta4_set):\n",
    "                    return  count_in < num_iter_in\n",
    "                def body_4(count_in, num_iter_in, delta3_set, delta4_set):\n",
    "                    delta3 = tf.reduce_sum(tf.square(tf.subtract(self.dat_alt3_ori[count_out], resize_gen_alt3[count_in])))\n",
    "                    delta4 = tf.reduce_sum(tf.square(tf.subtract(self.dat_alt4_ori[count_out], resize_gen_alt4[count_in])))\n",
    "                    delta3_set = tf.concat([delta3_set, [delta3]], 0)\n",
    "                    delta4_set = tf.concat([delta4_set, [delta4]], 0)\n",
    "\n",
    "                    count_in = count_in + 1\n",
    "                    return count_in, num_iter_in, delta3_set, delta4_set\n",
    "                count_in, num_iter_in, delta3_set, delta4_set = tf.while_loop(cond_4, body_4, \n",
    "                    [count_in, num_iter_in, delta3_set, delta4_set]\n",
    "                    , shape_invariants = [count_in.get_shape(), num_iter_in.get_shape(),\n",
    "                                          tf.TensorShape([None,]), tf.TensorShape([None,]) ]) \n",
    "\n",
    "                delta3_RE = tf.concat([delta3_RE, [tf.reduce_min(tf.stack(delta3_set))]], 0)\n",
    "                delta4_RE = tf.concat([delta4_RE, [tf.reduce_min(tf.stack(delta4_set))]], 0)            \n",
    "\n",
    "                count_out = count_out + 1\n",
    "                return count_out, num_iter_out, delta3_RE, delta4_RE\n",
    "            count_out, num_iter_out, delta3_RE, delta4_RE = tf.while_loop(cond_3, body_3, \n",
    "                [count_out, num_iter_out, delta3_RE, delta4_RE]\n",
    "                , shape_invariants=[count_out.get_shape(), num_iter_out.get_shape()\n",
    "                                    , tf.TensorShape([None,]), tf.TensorShape([None,]) ])     \n",
    "\n",
    "            MRE3 = tf.reduce_mean(delta3_RE)\n",
    "            MRE4 = tf.reduce_mean(delta4_RE)\n",
    "\n",
    "            MRE3_norm = tf.div(tf.abs(tf.subtract(self.MREvalue_val_alt3, MRE3)), self.MREvalue_val_alt3)\n",
    "            MRE4_norm = tf.div(tf.abs(tf.subtract(self.MREvalue_val_alt4, MRE4)), self.MREvalue_val_alt4)             \n",
    "\n",
    "            '''restrict all ranking sum to 1'''\n",
    "            with tf.variable_scope(\"sum1\", reuse=False):\n",
    "                count_sum1 = tf.constant(0) \n",
    "                num_iter_sum1 = tf.constant(self.batch_size)  \n",
    "                \n",
    "                resize_gen_sum1 = tf.Variable([])\n",
    "                def cond_sum1(count_sum1, num_iter_sum1, resize_gen_sum1):\n",
    "                    return count_sum1 < num_iter_sum1\n",
    "                def body_sum1(count_sum1, num_iter_sum1, resize_gen_sum1):\n",
    "                    resize3 = lambda: tf.reduce_sum(self._resize_fault3(self.fake_output_mre[count_sum1]))\n",
    "                    resize4 = lambda: tf.reduce_sum(self._resize_fault4(self.fake_output_mre[count_sum1]))\n",
    "                    resize_result = tf.cond(tf.reduce_mean(self.MRE_val_index[count_sum1]) > 0, resize4, resize3)\n",
    "                    resize_gen_sum1 = tf.concat([resize_gen_sum1, [resize_result]], 0)\n",
    "\n",
    "                    count_sum1 = count_sum1 + 1\n",
    "                    return count_sum1, num_iter_sum1, resize_gen_sum1\n",
    "                count_sum1, num_iter_sum1, resize_gen_sum1 = tf.while_loop(cond_sum1, body_sum1,\n",
    "                    [count_sum1, num_iter_sum1, resize_gen_sum1]\n",
    "                    , shape_invariants=[count_sum1.get_shape(), num_iter_sum1.get_shape(), tf.TensorShape([None,])])     \n",
    "\n",
    "                sum1 = tf.constant([1]*self.batch_size, shape=[self.batch_size,], dtype=tf.float32)\n",
    "                rank_delta_sum1 = tf.reduce_sum(tf.abs(tf.subtract(resize_gen_sum1, sum1)))        \n",
    "\n",
    "                rank_delta_sum1_ = tf.zeros([1,])\n",
    "                rank_delta_sum1_ = rank_delta_sum1_ + rank_delta_sum1\n",
    "                \n",
    "        return MRE3_norm, MRE4_norm, tf.reduce_mean(rank_delta_sum1_)\n",
    "\n",
    "        \n",
    "    def _object_cost_function(self):\n",
    "        # networks : generator\n",
    "        self.fake_output = self._GEN(self.Z, self.Y_label, self.keep_prob_feed, self.isTrain, _reuse=False)\n",
    "\n",
    "        # networks : discriminator\n",
    "        real_score = self._DIS(self.X, self.Y_fill, self.keep_prob_feed, self.isTrain, _reuse=False)\n",
    "        fake_score = self._DIS(self.fake_output, self.Y_fill, self.keep_prob_feed, self.isTrain, _reuse=True)\n",
    "\n",
    "        # Mean Recovery Error (MRE): calculate MRE after epoch is over 2000\n",
    "        self.dat_alt3_ori = tf.cast(self.dat_alt3_ori, dtype=tf.float32)\n",
    "        self.dat_alt4_ori = tf.cast(self.dat_alt4_ori, dtype=tf.float32)\n",
    "        self.fake_output = tf.cast(self.fake_output, dtype=tf.float32)\n",
    "        self.MREvalue_val_alt3 = tf.cast(self.MREvalue_val_alt3, dtype=tf.float32)\n",
    "        self.MREvalue_val_alt4 = tf.cast(self.MREvalue_val_alt4, dtype=tf.float32)\n",
    "        self.MRE_val_index = tf.cast(self.MRE_val_index, dtype=tf.float32)\n",
    "                \n",
    "        def f1(): return self._cal_MRE_and_delta_sum1(self.fake_output)\n",
    "        def f2(): return tf.cast(0, dtype=tf.float32), tf.cast(0, dtype=tf.float32), tf.cast(0, dtype=tf.float32)\n",
    "        self.MRE3_norm, self.MRE4_norm, self.rank_delta_sum1 = tf.cond(\n",
    "            tf.greater(self.global_step, self.MemorizationPenalty_start), f1, f2)\n",
    "        \n",
    "        '''define the loss ops'''\n",
    "        self.D_loss = tf.reduce_mean(-real_score + fake_score)\n",
    "#         self.G_loss = tf.reduce_mean(-fake_score) + 0.5*(self.MRE3_norm + self.MRE4_norm) + 0.5*self.rank_delta_sum1\n",
    "                \n",
    "        def f3(): return tf.reduce_mean(-fake_score)\n",
    "        def f4(): return tf.reduce_mean(-fake_score) + 0.5*(self.MRE3_norm + self.MRE4_norm) + 0.5*self.rank_delta_sum1\n",
    "        self.G_loss = tf.cond(tf.mod(self.n_critic, self.MemorizationPenalty_number) > 0, f3, f4)        \n",
    "        \n",
    "            \n",
    "        ################  Gradient penalty  ################\n",
    "        LAMBDA = 10\n",
    "        alpha = tf.random_uniform(shape=[self.batch_size, 1], minval=0.,maxval=1.)  \n",
    "\n",
    "        real_data = tf.reshape(self.X, (self.batch_size, self.dim_height*self.dim_width))    \n",
    "        fake_data = tf.reshape(self.fake_output, (self.batch_size, self.dim_height*self.dim_width))    \n",
    "\n",
    "        interpolates = (alpha * real_data + ((1 - alpha) * fake_data))    \n",
    "        interpolates_d = tf.reshape(interpolates, (self.batch_size, self.dim_height, self.dim_width, 1))  \n",
    "        interpolates_d = self._DIS(interpolates_d, self.Y_fill, self.keep_prob_feed, self.isTrain, _reuse=True)                   \n",
    "\n",
    "        gradients = tf.gradients(interpolates_d, [interpolates])[0]   \n",
    "        slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))  \n",
    "        gradient_penalty = tf.reduce_mean((slopes-1.)**2) \n",
    "        self.D_loss = self.D_loss + LAMBDA*gradient_penalty\n",
    "        ###############  Gradient penalty  ################\n",
    "\n",
    "\n",
    "        # define the optimizer ops\n",
    "        self.T_vars = tf.trainable_variables()\n",
    "#         self.D_vars = [var for var in self.T_vars if var.name.startswith('DIS')]\n",
    "#         self.G_vars = [var for var in self.T_vars if var.name.startswith('GEN')]\n",
    "        self.D_vars = [var for var in self.T_vars if 'DIS' in var.name]\n",
    "        self.G_vars = [var for var in self.T_vars if 'DIS' not in var.name]\n",
    "                \n",
    "        learning_rate = 0.0001\n",
    "        # define the update ops to run batch normalization\n",
    "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "#             self.D_optim = tf.train.RMSPropOptimizer(learning_rate).minimize(self.D_loss, var_list=self.D_vars)\n",
    "#             self.G_optim = tf.train.RMSPropOptimizer(learning_rate).minimize(self.G_loss, var_list=self.G_vars)\n",
    "            self.D_optim = tf.train.AdamOptimizer(learning_rate, beta1=0.5, beta2=0.9).minimize(self.D_loss, var_list=self.D_vars)\n",
    "            self.G_optim = tf.train.AdamOptimizer(learning_rate, beta1=0.5, beta2=0.9).minimize(self.G_loss, var_list=self.G_vars)\n",
    "            \n",
    "            \n",
    "    def train_model(self,\n",
    "                    x_train = None,     \n",
    "                    y_train = None,     \n",
    "                    keep_prob = None,\n",
    "                    train_epoch = None, \n",
    "                    n_critic = None,\n",
    "                    step_valid = 50,\n",
    "                    step_save_data = 1000,\n",
    "                    iteration_generator = 50,\n",
    "                    MREvalue_val_alt3 = None,\n",
    "                    MREvalue_val_alt4 = None,\n",
    "                    dat_alt3_ori = None,\n",
    "                    dat_alt4_ori = None,\n",
    "                    MemorizationPenalty_number = None,\n",
    "                   ): \n",
    "        \n",
    "        self.train_hist = {}\n",
    "        self.train_hist['D_losses'] = []\n",
    "        self.train_hist['G_losses'] = []\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "#         config = tf.ConfigProto(gpu_options = tf.GPUOptions(allow_growth=True))\n",
    "\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = 0.1 # use 90% memory of GPU\n",
    "\n",
    "        with tf.Session(config = config) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print('Optimization start!')\n",
    "            \n",
    "            for epoch in range(train_epoch):\n",
    "                G_losses = []\n",
    "                D_losses = []\n",
    "                \n",
    "                time_start_epoch = time.time()\n",
    "                \n",
    "                for i in range(len(x_train) // self.batch_size): # num_samples / batch_size\n",
    "                    '''#############        Discriminator       #######################'''\n",
    "                    x_ = x_train[i*self.batch_size: (i+1)*self.batch_size] \n",
    "                    x_ = x_.reshape((self.batch_size, self.dim_height, self.dim_width, 1))   #(batch_size, dim_height, dim_width, 1)\n",
    "                    y_label_ = y_train[i*self.batch_size:(i+1)*self.batch_size].reshape([self.batch_size, 1, 1, self.num_class])\n",
    "                    y_fill_ = y_label_ * np.ones([self.batch_size, self.dim_height, self.dim_width, self.num_class]) #(batch_szie, height, width, num_class)\n",
    "                    z_ = np.random.normal(0, 1, (self.batch_size, 1, 1, self.dim_z))\n",
    "\n",
    "                    loss_d, _ = sess.run([self.D_loss, self.D_optim], \n",
    "                                          feed_dict={self.X: x_, \n",
    "                                                     self.Z: z_, \n",
    "                                                     self.Y_fill: y_fill_, \n",
    "                                                     self.Y_label: y_label_, \n",
    "                                                     self.keep_prob_feed: keep_prob,\n",
    "                                                     self.isTrain: True,\n",
    "                                                     self.global_step: epoch})\n",
    "                    D_losses.append(loss_d)\n",
    "\n",
    "                    '''#############        Generator          #######################'''\n",
    "                    if (i+1) % n_critic == 0:   # Train the generator every n_critic iterations\n",
    "                        z_ = np.random.normal(0, 1, (self.batch_size, 1, 1, self.dim_z))\n",
    "                        y_ = []\n",
    "                        for _ in range(self.batch_size): y_.append(random.randrange(0, self.num_class, self.num_class-1))  \n",
    "                        y_ = np.array(y_).reshape([self.batch_size, 1])\n",
    "        #                 y_ = np.random.randint(0, 2, (batch_size, 1))\n",
    "                        y_label_ = one_hot(y_, self.num_class).reshape([self.batch_size, 1, 1, self.num_class])\n",
    "                        y_fill_ = y_label_ * np.ones([self.batch_size, self.dim_height, self.dim_width, self.num_class])\n",
    "                        \n",
    "                        random_seed = np.random.randint(len(dat_alt3_ori), size = self.batch_size)\n",
    "                        dat_alt3_ori_UniformSampling = dat_alt3_ori[random_seed]\n",
    "                        dat_alt4_ori_UniformSampling = dat_alt4_ori[random_seed]\n",
    "                        \n",
    "                        loss_g, _, MRE3_norm, MRE4_norm, rank_delta_sum1 = sess.run([\n",
    "                            self.G_loss, self.G_optim, self.MRE3_norm, self.MRE4_norm, self.rank_delta_sum1], \n",
    "                                              feed_dict={self.Z: z_, \n",
    "                                                         self.X: x_, \n",
    "                                                         self.Y_fill: y_fill_, \n",
    "                                                         self.Y_label: y_label_, \n",
    "                                                         self.keep_prob_feed: keep_prob,\n",
    "                                                         self.isTrain: True,\n",
    "                                                         self.MREvalue_val_alt3: MREvalue_val_alt3,\n",
    "                                                         self.MREvalue_val_alt4: MREvalue_val_alt4,\n",
    "                                                         self.MRE_val_index: y_,\n",
    "                                                         self.dat_alt3_ori: dat_alt3_ori_UniformSampling,\n",
    "                                                         self.dat_alt4_ori: dat_alt4_ori_UniformSampling,\n",
    "                                                         self.global_step: epoch,\n",
    "                                                         self.n_critic: n_critic,\n",
    "                                                         self.MemorizationPenalty_number: MemorizationPenalty_number,\n",
    "                                                        })\n",
    "                        G_losses.append(loss_g)\n",
    "#                         print('MRE3_norm ', MRE3_norm)\n",
    "#                         print('MRE4_norm ', MRE4_norm)\n",
    "#                         print('rank_delta_sum1 ', rank_delta_sum1)\n",
    "\n",
    "            \n",
    "            ############        print result      #######################\n",
    "                if (epoch+1) % 1 == 0:\n",
    "                    print('[%d/%d] loss_d: %.3f, loss_g: %.3f'%((epoch + 1), train_epoch, np.mean(D_losses), np.mean(G_losses)))\n",
    "                    self.train_hist['D_losses'].append(np.mean(D_losses))\n",
    "                    self.train_hist['G_losses'].append(np.mean(G_losses))\n",
    "\n",
    "            ############        valid      #######################\n",
    "                if (epoch+1) % step_valid == 0:\n",
    "                   ############        num_alternative = 3      #######################\n",
    "                    z_ = np.random.normal(0, 1, (self.batch_size, 1, 1, self.dim_z))   \n",
    "\n",
    "                    y_ = np.zeros([self.batch_size, 1]) + 0\n",
    "                    y_label_ = one_hot(y_, self.num_class).reshape([self.batch_size, 1, 1, self.num_class])\n",
    "                    y_fill_ = y_label_ * np.ones([self.batch_size, self.dim_height, self.dim_width, self.num_class])\n",
    "                    output_g_alt3 = sess.run([self.fake_output], \n",
    "                                          feed_dict={self.Z: z_, \n",
    "                                                     self.X: x_, \n",
    "                                                     self.Y_fill: y_fill_, \n",
    "                                                     self.Y_label: y_label_, \n",
    "                                                     self.keep_prob_feed: 1,\n",
    "                                                     self.isTrain: False})    \n",
    "                    output_g_alt3_ = np.array(output_g_alt3).reshape([self.batch_size, self.dim_height, self.dim_width])[0]\n",
    "\n",
    "                    ############        num_alternative = 4      #######################\n",
    "                    z_ = np.random.normal(0, 1, (self.batch_size, 1, 1, self.dim_z))   \n",
    "\n",
    "                    y_ = np.zeros([self.batch_size, 1]) + 1\n",
    "                    y_label_ = one_hot(y_, self.num_class).reshape([self.batch_size, 1, 1, self.num_class])\n",
    "                    y_fill_ = y_label_ * np.ones([self.batch_size, self.dim_height, self.dim_width, self.num_class])\n",
    "                    output_g_alt4 = sess.run([self.fake_output], \n",
    "                                          feed_dict={self.Z: z_, \n",
    "                                                     self.X: x_, \n",
    "                                                     self.Y_fill: y_fill_, \n",
    "                                                     self.Y_label: y_label_, \n",
    "                                                     self.keep_prob_feed: 1,\n",
    "                                                     self.isTrain: False})  \n",
    "                    output_g_alt4_ = np.array(output_g_alt4).reshape([self.batch_size, self.dim_height, self.dim_width])[0]\n",
    "                                     \n",
    "                    plt.figure(epoch)\n",
    "                    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "                    ax.cla()\n",
    "                    ax.imshow(np.reshape(output_g_alt3_, (self.dim_height, self.dim_width)), cmap='gray')\n",
    "\n",
    "                    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "                    ax.cla()\n",
    "                    ax.imshow(np.reshape(output_g_alt4_, (self.dim_height, self.dim_width)), cmap='gray')            \n",
    "                    plt.show()\n",
    "\n",
    "            ############        save per 1000 epoch      #######################\n",
    "                if (epoch+1) % step_save_data == 0:\n",
    "                    \n",
    "                    generated_3alt = []\n",
    "                    generated_4alt = []                     \n",
    "                    for _ in range(iteration_generator):  \n",
    "                         ############        num_alternative = 3      #######################\n",
    "                        z_ = np.random.normal(0, 1, (self.batch_size, 1, 1, self.dim_z))   \n",
    "                        y_ = np.zeros([self.batch_size, 1]) + 0\n",
    "                        y_label_ = one_hot(y_, self.num_class).reshape([self.batch_size, 1, 1, self.num_class])\n",
    "\n",
    "                        output_g_alt3 = sess.run([self.fake_output], feed_dict={self.Z: z_, self.Y_label: y_label_})    \n",
    "                        generated_3alt.append(np.array(output_g_alt3).reshape([self.batch_size, self.dim_height, self.dim_width]))\n",
    "\n",
    "                        ############        num_alternative = 4      #######################\n",
    "                        z_ = np.random.normal(0, 1, (self.batch_size, 1, 1, self.dim_z))   \n",
    "                        y_ = np.zeros([self.batch_size, 1]) + 1\n",
    "                        y_label_ = one_hot(y_, self.num_class).reshape([self.batch_size, 1, 1, self.num_class])\n",
    "\n",
    "                        output_g_alt4 = sess.run([self.fake_output], feed_dict={self.Z: z_, self.Y_label: y_label_})  \n",
    "                        generated_4alt.append(np.array(output_g_alt4).reshape([self.batch_size, self.dim_height, self.dim_width]))\n",
    "\n",
    "                    generated_3alt = np.array(generated_3alt).reshape([iteration_generator*self.batch_size, self.dim_height, self.dim_width])\n",
    "                  \n",
    "                    gen_alt3_ori = resize_to_ori(generated_3alt, np.math.factorial(6), 30, 24, np.math.factorial(3), self.batch_size, iteration_generator)\n",
    "                    gen_alt3_pd = pd.DataFrame(gen_alt3_ori, columns = ['ABC', 'ACB', 'BAC', 'BCA', 'CAB', 'CBA'])\n",
    "                    gen_alt3_pd.to_csv(generated_path + 'generated_atl3_' + str(epoch) + '.csv')                    \n",
    "                    \n",
    "                    generated_4alt = np.array(generated_4alt).reshape([iteration_generator*self.batch_size, self.dim_height, self.dim_width])     \n",
    "                    \n",
    "                    gen_alt4_ori = resize_to_ori(generated_4alt, np.math.factorial(6), 30, 24, np.math.factorial(4), self.batch_size, iteration_generator)\n",
    "                    gen_alt4_pd = pd.DataFrame(gen_alt4_ori, columns = ['ABCD', 'ACBD', 'BACD', 'BCAD', 'CABD', 'CBAD', 'DABC',\n",
    "                           'DACB', 'DBAC', 'DBCA', 'DCAB', 'DCBA', 'ADBC', 'ADCB', 'BDAC', 'BDCA',\n",
    "                           'CDAB', 'CDBA', 'ABDC', 'ACDB', 'BADC', 'BCDA', 'CADB', 'CBDA'])\n",
    "#                     print(gen_alt4_pd.shape)\n",
    "                    gen_alt4_pd.to_csv(generated_path + 'generated_atl4_' + str(epoch) + '.csv')   \n",
    "                    \n",
    "\n",
    "                time_end_epoch = time.time()\n",
    "                print('Time cost in one epoch', time_end_epoch - time_start_epoch,'s') \n",
    "                \n",
    "            ###########        save      #######################\n",
    "            saver.save(sess, saver_path)   \n",
    "            print('save success')\n",
    "\n",
    "            sess.close()\n",
    "            \n",
    "        print(\"Optimization Finished!\")\n",
    "        \n",
    "    '''loss curve'''\n",
    "    def show_train_hist(self):\n",
    "        x = range(len(self.train_hist['D_losses']))\n",
    "\n",
    "        y1 = self.train_hist['D_losses']\n",
    "        y2 = self.train_hist['G_losses']\n",
    "\n",
    "        plt.plot(x, y1, label='D_loss')\n",
    "        plt.plot(x, y2, label='G_loss')\n",
    "\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "\n",
    "        plt.legend(loc=4)  \n",
    "        plt.grid(True)\n",
    "        plt.tight_layout() \n",
    "        plt.title(\"Training Losses\")\n",
    "\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c99b20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = data_alt_3_4.shape[0]\n",
    "dim_height = data_alt_3_4.shape[1]\n",
    "dim_width = data_alt_3_4.shape[2]\n",
    "dim_z = 128\n",
    "num_class = label_alt_onehot.shape[-1]\n",
    "batch_size = 50\n",
    "MemorizationPenalty_start = 2000 #2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "799334a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = election_GAN(\n",
    "                num_samples = num_samples,\n",
    "                dim_height = dim_height,\n",
    "                dim_width = dim_width,\n",
    "                dim_z = dim_z,\n",
    "                num_class = num_class,\n",
    "                batch_size = batch_size,\n",
    "                MemorizationPenalty_start = MemorizationPenalty_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c660c1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization start!\n",
      "[1/30] loss_d: 2.157, loss_g: 0.647\n",
      "Time cost in one epoch 4.666980504989624 s\n",
      "[2/30] loss_d: -1.822, loss_g: 3.381\n",
      "Time cost in one epoch 2.559800624847412 s\n",
      "[3/30] loss_d: -2.526, loss_g: 4.926\n",
      "Time cost in one epoch 2.563100576400757 s\n",
      "[4/30] loss_d: -2.822, loss_g: 4.590\n",
      "Time cost in one epoch 2.5676982402801514 s\n",
      "[5/30] loss_d: -2.856, loss_g: 4.037\n",
      "Time cost in one epoch 2.560779094696045 s\n",
      "[6/30] loss_d: -2.899, loss_g: 3.722\n",
      "Time cost in one epoch 2.567086696624756 s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-eda06fef9351>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mdat_alt3_ori\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_alt3_ori\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mdat_alt4_ori\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_alt4_ori\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m24\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mMemorizationPenalty_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# penalty (num_samples (2000)//batch_size (50)//MemorizationPenalty_number(10) ) times per epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     )\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-d8f99aaaaed1>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self, x_train, y_train, keep_prob, train_epoch, n_critic, step_valid, step_save_data, iteration_generator, MREvalue_val_alt3, MREvalue_val_alt4, dat_alt3_ori, dat_alt4_ori, MemorizationPenalty_number)\u001b[0m\n\u001b[0;32m    385\u001b[0m                                                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_prob_feed\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m                                                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misTrain\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m                                                      self.global_step: epoch})\n\u001b[0m\u001b[0;32m    388\u001b[0m                     \u001b[0mD_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1.12\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1.12\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1.12\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1.12\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1.12\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1.12\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time_start=time.time()\n",
    "\n",
    "model.train_model(\n",
    "    x_train = data_alt_3_4,     \n",
    "    y_train = label_alt_onehot,     \n",
    "    keep_prob = 0.5, #0.5\n",
    "    train_epoch = 30, #17000\n",
    "    n_critic = 5,#5\n",
    "    step_valid = 50, #50\n",
    "    step_save_data = 1000, #1000\n",
    "    iteration_generator = 50,\n",
    "    MREvalue_val_alt3 = MREvalue_val_alt3,\n",
    "    MREvalue_val_alt4 = MREvalue_val_alt4,\n",
    "    dat_alt3_ori = data_alt3_ori.reshape([-1, 6]),\n",
    "    dat_alt4_ori = data_alt4_ori.reshape([-1, 24]),\n",
    "    MemorizationPenalty_number = 1, # penalty (num_samples (2000)//batch_size (50)//MemorizationPenalty_number(10) ) times per epoch\n",
    "    )\n",
    "\n",
    "time_end=time.time()\n",
    "print('Total Time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ed8ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.show_train_hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828aba7c",
   "metadata": {},
   "source": [
    "# Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81da2d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class election_GAN_GEN(object):\n",
    "    def __init__(self,\n",
    "                dim_height = None,\n",
    "                dim_width = None,\n",
    "                dim_z = None,\n",
    "                num_class = None,\n",
    "                batch_size = None,):\n",
    "        \n",
    "        # Definition Params:\n",
    "        self.dim_height = dim_height    \n",
    "        self.dim_width = dim_width  \n",
    "        self.dim_z = dim_z    \n",
    "        self.num_class = num_class      \n",
    "        self.batch_size = batch_size  \n",
    "\n",
    "        # Define Network Input:\n",
    "        self.Z = tf.placeholder(tf.float32, shape=(None, 1, 1, self.dim_z))\n",
    "        self.Y_label = tf.placeholder(tf.float32, shape=(None, 1, 1, self.num_class))\n",
    "\n",
    "        # Network:\n",
    "        self._GEN(self.Z, self.Y_label)\n",
    "\n",
    "        # get the generated data\n",
    "        with tf.variable_scope('object_cost_function', reuse=tf.AUTO_REUSE):\n",
    "            self.call_GEN()\n",
    "        \n",
    "    def _GEN(self, Z, Y_label):            \n",
    "        keep_prob_feed = 1\n",
    "        isTrain=False \n",
    "        dim = 128\n",
    "        \n",
    "        with tf.variable_scope(\"GEN\", reuse=False):\n",
    "            # concat layer\n",
    "            Z_ = tf.concat([Z, Y_label], 3)  #(batch_szie, 1, 1, dim_z + num_class)\n",
    "            Z_ = tf.reshape(Z_, (-1, self.dim_z + self.num_class))        \n",
    "\n",
    "            # FCN \n",
    "            hidden = tf.layers.dense(Z_, units = 2*2*3*dim)\n",
    "#             hidden = tf.layers.batch_normalization(hidden, training=isTrain)\n",
    "            hidden = tf.nn.relu(hidden)\n",
    "            hidden = tf.reshape(hidden, (-1, 2, 2, 3*dim)) \n",
    "\n",
    "            # CNN 1 \n",
    "            hidden = tf.layers.conv2d_transpose(hidden, 2*dim, [3, 3], strides=(1, 2), padding='valid')\n",
    "#             hidden = tf.layers.batch_normalization(hidden, training=isTrain)\n",
    "            hidden = tf.nn.relu(hidden)\n",
    "\n",
    "            # CNN 2 \n",
    "            hidden = tf.layers.conv2d_transpose(hidden, 1*dim, [3, 3], strides=(3, 3), padding='same')\n",
    "#             hidden = tf.layers.batch_normalization(hidden, training=isTrain)\n",
    "            hidden = tf.nn.relu(hidden)     \n",
    "\n",
    "            # CNN 3 \n",
    "            hidden = tf.layers.conv2d_transpose(hidden, 1, [3, 3], strides=(2, 2), padding='same')         \n",
    "            output = hidden\n",
    "\n",
    "            return output     \n",
    "    \n",
    "    def call_GEN(self):\n",
    "        self.fake_output = self._GEN(self.Z, self.Y_label)\n",
    "\n",
    "    def GEN_model(self, path, iteration): \n",
    "    \n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "            saver.restore(sess, tf.train.latest_checkpoint(path))      \n",
    "            \n",
    "            generated_3alt = []\n",
    "            generated_4alt = []  \n",
    "            \n",
    "            for _ in range(iteration): \n",
    "                '''#############        Generator          #######################'''\n",
    "                ############        num_alternative = 3      #######################\n",
    "                z_ = np.random.normal(0, 1, (self.batch_size, 1, 1, self.dim_z))   \n",
    "                y_ = np.zeros([self.batch_size, 1]) + 0\n",
    "                y_label_ = one_hot(y_, self.num_class).reshape([self.batch_size, 1, 1, self.num_class])\n",
    "\n",
    "                output_g_alt3 = sess.run([self.fake_output], feed_dict={self.Z: z_, self.Y_label: y_label_})    \n",
    "                output_g_alt3_ = np.array(output_g_alt3).reshape([self.batch_size, self.dim_height, self.dim_width])\n",
    "                generated_3alt.append(output_g_alt3_)\n",
    "                \n",
    "                ############        num_alternative = 4      #######################\n",
    "                z_ = np.random.normal(0, 1, (self.batch_size, 1, 1, self.dim_z))   \n",
    "                y_ = np.zeros([self.batch_size, 1]) + 1\n",
    "                y_label_ = one_hot(y_, self.num_class).reshape([self.batch_size, 1, 1, self.num_class])\n",
    "\n",
    "                output_g_alt4 = sess.run([self.fake_output], feed_dict={self.Z: z_, self.Y_label: y_label_})  \n",
    "                output_g_alt4_ = np.array(output_g_alt4).reshape([self.batch_size, self.dim_height, self.dim_width])\n",
    "                generated_4alt.append(output_g_alt4_)\n",
    "\n",
    "            generated_3alt = np.array(generated_3alt).reshape([iteration*self.batch_size, self.dim_height, self.dim_width])\n",
    "            generated_4alt = np.array(generated_4alt).reshape([iteration*self.batch_size, self.dim_height, self.dim_width])\n",
    "                \n",
    "            return generated_3alt, generated_4alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87163b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c002f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = election_GAN_GEN(\n",
    "                dim_height = dim_height,\n",
    "                dim_width = dim_width,\n",
    "                dim_z = dim_z,\n",
    "                num_class = num_class,\n",
    "                batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f77d2d3",
   "metadata": {},
   "source": [
    "shape[0]: number of generated data\n",
    "\n",
    "shape[1], shape[2]: data size before resize back to original size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d5d988",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_generator = 50\n",
    "gen_alt3, gen_alt4 = generator.GEN_model(path = restore_path, \n",
    "                                         iteration = iteration_generator) #iteration: how many times generator is used?\n",
    "print(gen_alt3.shape) \n",
    "print(gen_alt4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12273be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.cla()\n",
    "ax.imshow(np.reshape(gen_alt3[0], (dim_height, dim_width)), cmap='gray')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.cla()\n",
    "ax.imshow(np.reshape(gen_alt4[0], (dim_height, dim_width)), cmap='gray')            \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e973dce",
   "metadata": {},
   "source": [
    "# Resize to original shpe & Save as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84d24d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = np.math.factorial(6)\n",
    "img_width = 30\n",
    "img_height = 24\n",
    "ori_size = np.math.factorial(3)\n",
    "\n",
    "gen_alt3_ori = resize_to_ori(gen_alt3, img_size, img_width, img_height, ori_size, batch_size, iteration_generator)\n",
    "gen_alt3_pd = pd.DataFrame(gen_alt3_ori, columns = ['ABC', 'ACB', 'BAC', 'BCA', 'CAB', 'CBA'])\n",
    "print(gen_alt3_pd.shape)\n",
    "gen_alt3_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266b6adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_alt3_pd.to_csv(generated_path + 'generated_atl3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f74596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = np.math.factorial(6)\n",
    "img_width = 30\n",
    "img_height = 24\n",
    "ori_size = np.math.factorial(4)\n",
    "\n",
    "gen_alt4_ori = resize_to_ori(gen_alt4, img_size, img_width, img_height, ori_size, batch_size, iteration_generator)\n",
    "gen_alt4_pd = pd.DataFrame(gen_alt4_ori, columns = ['ABCD', 'ACBD', 'BACD', 'BCAD', 'CABD', 'CBAD', 'DABC',\n",
    "       'DACB', 'DBAC', 'DBCA', 'DCAB', 'DCBA', 'ADBC', 'ADCB', 'BDAC', 'BDCA',\n",
    "       'CDAB', 'CDBA', 'ABDC', 'ACDB', 'BADC', 'BCDA', 'CADB', 'CBDA'])\n",
    "print(gen_alt4_pd.shape)\n",
    "gen_alt4_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feab2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_alt4_pd.to_csv(generated_path + 'generated_atl4.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.12",
   "language": "python",
   "name": "tf1.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
