{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14823bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WIN10\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time, random\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import one_hot, resize_to_ori_calMRE, resize_to_ori\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182b7aca",
   "metadata": {},
   "source": [
    "# Define save path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4af1512",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'election_GAN(pytorch)/'\n",
    "gen_results = 'generated_results/'\n",
    "\n",
    "model = 'model_'\n",
    "\n",
    "if not os.path.isdir(folder):\n",
    "    os.mkdir(folder)\n",
    "    \n",
    "if not os.path.isdir(folder + gen_results):\n",
    "    os.mkdir(folder + gen_results)\n",
    "\n",
    "# save ckpt\n",
    "saver_path = os.path.join(folder, model)\n",
    "\n",
    "# save generated data\n",
    "generated_path = os.path.join(folder + gen_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbfa907",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61daac8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 24, 30)\n",
      "(1000, 24, 30)\n",
      "(2000, 24, 30)\n"
     ]
    }
   ],
   "source": [
    "img_width = 30\n",
    "img_height = 24\n",
    "\n",
    "data_alt3 = pd.read_csv('./data/netflix_data_3alt_resize.csv')\n",
    "data_alt3 = data_alt3.iloc[:1000,1:].values\n",
    "\n",
    "data_alt3 = data_alt3.reshape([-1, img_height, img_width])\n",
    "print(data_alt3.shape)\n",
    "\n",
    "##################################\n",
    "data_alt4 = pd.read_csv('./data/netflix_data_4alt_resize.csv')\n",
    "data_alt4 = data_alt4.iloc[:1000,1:].values\n",
    "\n",
    "data_alt4 = data_alt4.reshape([-1, img_height, img_width])\n",
    "print(data_alt4.shape)\n",
    "\n",
    "data_alt_3_4 = np.concatenate( (data_alt3, data_alt4), axis = 0)\n",
    "print(data_alt_3_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20698c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(5, 5))\n",
    "# ax.cla()\n",
    "# ax.imshow(np.reshape(data_alt3[2], (data_alt3.shape[1], data_alt3.shape[2])), cmap='gray')\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(5, 5))\n",
    "# ax.cla()\n",
    "# ax.imshow(np.reshape(data_alt4[99], (data_alt3.shape[1], data_alt3.shape[2])), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b209522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2)\n"
     ]
    }
   ],
   "source": [
    "label_alt3 = np.zeros([data_alt3.shape[0]])\n",
    "label_alt4 = np.zeros([data_alt4.shape[0]]) + 1\n",
    "label_alt_3_4 = np.concatenate( (label_alt3,label_alt4), axis = 0)\n",
    "\n",
    "label_alt_onehot = one_hot(label_alt_3_4, 1 + 1)   \n",
    "print(label_alt_onehot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06608d70",
   "metadata": {},
   "source": [
    "# Define Generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57caff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(dis, x_real, x_fake, x_label, device=\"cpu\"):\n",
    "    batch_size, height, width = x_real.shape\n",
    "    alpha = torch.rand((batch_size, 1, 1)).repeat(1, height, width).to(device)\n",
    "    interpolated_images = x_real * alpha + x_fake * (1 - alpha)\n",
    "\n",
    "    # Calculate dis scores\n",
    "    mixed_scores = dis(interpolated_images, x_label)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55a7623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 128, num_classes = 2):\n",
    "        super(Generator, self).__init__()\n",
    "        self.dim = 128\n",
    "        self.hidden = torch.nn.Linear(z_dim + num_classes, self.dim*1*1*3)   \n",
    "        self.tconv1 = self.conv_block(self.dim*3, self.dim*3, k_size=[1,5], stride=[1,5], pad=0, out_pad=0, use_bn=False)\n",
    "        self.tconv2 = self.conv_block(self.dim*3, self.dim*2, k_size=[4,1], stride=[4,1], pad=0, out_pad=0, use_bn=False)\n",
    "        self.tconv3 = self.conv_block(self.dim*2, self.dim*1, k_size=[3,3], stride=[3,3], pad=0, out_pad=0, use_bn=False)\n",
    "        self.tconv4 = self.conv_block(self.dim*1, 1, k_size=[2,2], stride=[2,2], pad=0, out_pad=0, use_bn=False)        \n",
    "        \n",
    "    def conv_block(self, c_in, c_out, k_size, stride, pad, out_pad, use_bn=False):\n",
    "        module = []\n",
    "        module.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, padding=pad, output_padding=out_pad, bias=not use_bn))\n",
    "        if use_bn: module.append(nn.BatchNorm2d(c_out))\n",
    "        return nn.Sequential(*module)        \n",
    "\n",
    "    def forward(self, x, label):\n",
    "        x = x.reshape([x.shape[0], -1, 1, 1])\n",
    "        label = label.reshape([x.shape[0], -1, 1, 1])\n",
    "        x = torch.cat((x, label), dim=1).reshape([x.shape[0], -1])\n",
    "        x = F.relu(self.hidden(x)).reshape([x.shape[0], 3*self.dim, 1, 1])\n",
    "        x = F.relu(self.tconv1(x))\n",
    "        x = F.relu(self.tconv2(x))\n",
    "        x = self.tconv3(x)\n",
    "        x = self.tconv4(x)\n",
    "        \n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22344574",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''padding: same'''\n",
    "\n",
    "class Conv2dSame(torch.nn.Conv2d):\n",
    "\n",
    "    def calc_same_pad(self, i: int, k: int, s: int, d: int) -> int:\n",
    "        return max((math.ceil(i / s) - 1) * s + (k - 1) * d + 1 - i, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ih, iw = x.size()[-2:]\n",
    "\n",
    "        pad_h = self.calc_same_pad(i=ih, k=self.kernel_size[0], s=self.stride[0], d=self.dilation[0])\n",
    "        pad_w = self.calc_same_pad(i=iw, k=self.kernel_size[1], s=self.stride[1], d=self.dilation[1])\n",
    "\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            x = F.pad(\n",
    "                x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2]\n",
    "            )\n",
    "        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes = 2):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.dim = 128\n",
    "        self.height = 24\n",
    "        self.width = 30\n",
    "        self.conv1 = self.conv_block(1 + num_classes, self.dim, k_size=[4,5], stride=[2,2], pad=0)\n",
    "        self.conv2 = self.conv_block(self.dim, self.dim * 2, k_size=[4,5], stride=[2,2], pad=0)\n",
    "        self.conv3 = self.conv_block(self.dim * 2, self.dim * 2, k_size=[4,5], stride=[2,2], pad=0)\n",
    "        self.hidden = torch.nn.Linear(3072, 1)  \n",
    "\n",
    "    def conv_block(self, c_in, c_out, k_size, stride, pad=0, use_bn=False):\n",
    "        module = []\n",
    "        module.append(Conv2dSame(in_channels=c_in, \n",
    "                                 out_channels=c_out, kernel_size=k_size, stride=stride, groups=1, bias=not use_bn))\n",
    "        if use_bn: module.append(nn.BatchNorm2d(c_out))\n",
    "        return nn.Sequential(*module)        \n",
    "        \n",
    "    def forward(self, x, label):\n",
    "        alpha = 0.2\n",
    "        x = x.reshape([batch_size, 1, self.height, self.width])\n",
    "        label = label.reshape([batch_size, 2, self.height, self.width])\n",
    "        x = torch.cat((x, label), dim=1)\n",
    "        x = F.leaky_relu(self.conv1(x), alpha)\n",
    "        x = F.leaky_relu(self.conv2(x), alpha)\n",
    "        x = self.conv3(x)\n",
    "        x = x.reshape([x.shape[0], -1])\n",
    "        x = self.hidden(x)\n",
    "                        \n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e3e17ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class election_GAN(object):\n",
    "    def __init__(self,\n",
    "                num_samples = None,\n",
    "                dim_height = None,\n",
    "                dim_width = None,\n",
    "                dim_z = None,\n",
    "                num_class = None,\n",
    "                batch_size = None,\n",
    "                lambda_gp = None):\n",
    "        \n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#         self.device = \"cpu\" \n",
    "        self.num_samples = num_samples\n",
    "        self.dim_height = dim_height\n",
    "        self.dim_width = dim_width\n",
    "        self.dim_z = dim_z\n",
    "        self.num_class = num_class\n",
    "        self.batch_size = batch_size\n",
    "        self.lambda_gp = lambda_gp\n",
    "       \n",
    "    def train_model(self,\n",
    "                    x_train = None,     \n",
    "                    y_train = None,\n",
    "                    dataloder_num_workers = 1,\n",
    "                    train_epoch = 17000, \n",
    "                    step_valid = 50,\n",
    "                    step_save_data = 1000,\n",
    "                    iteration_generator = None,\n",
    "                    n_critic = 5,\n",
    "                   ):\n",
    "        '''define gen, dis and optim'''\n",
    "        self.gen = Generator(z_dim = self.dim_z, num_classes = self.num_class).to(self.device)\n",
    "        self.dis = Discriminator(num_classes = self.num_class).to(self.device)\n",
    "\n",
    "        self.g_opt = optim.Adam(self.gen.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "        self.d_opt = optim.Adam(self.dis.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "        \n",
    "        '''load data into dataloder'''\n",
    "        torch_dataset = Data.TensorDataset(torch.Tensor(x_train), torch.Tensor(y_train))\n",
    "\n",
    "        data_loader = Data.DataLoader(dataset = torch_dataset, batch_size = self.batch_size, \n",
    "                                shuffle = True, num_workers = dataloder_num_workers, drop_last = True)\n",
    "        \n",
    "        \n",
    "        ''' Training '''\n",
    "        self.train_hist = {}\n",
    "        self.train_hist['D_losses'] = []\n",
    "        self.train_hist['G_losses'] = []\n",
    "\n",
    "        time_start=time.time()\n",
    "\n",
    "        print('Optimization start!')\n",
    "        for epoch in range(train_epoch):\n",
    "            G_losses = []\n",
    "            D_losses = []\n",
    "\n",
    "            time_start_epoch = time.time()\n",
    "\n",
    "            for i, data in enumerate(data_loader):\n",
    "                x_real, x_label = data        \n",
    "                x_real = x_real.to(self.device)\n",
    "\n",
    "\n",
    "                z_label = x_label.reshape([self.batch_size, self.num_class, 1, 1]).to(self.device)\n",
    "                np_ones = np.ones([self.batch_size, self.num_class, self.dim_height, self.dim_width])\n",
    "                x_label = z_label * torch.Tensor(np_ones).to(self.device)\n",
    "\n",
    "                '''#############        Discriminator       #######################'''\n",
    "                z_fake = torch.randn(self.batch_size, self.dim_z, 1, 1).to(self.device)\n",
    "                x_fake = self.gen(z_fake, z_label)\n",
    "                fake_out = self.dis(x_fake.detach(), x_label)\n",
    "                real_out = self.dis(x_real.detach(), x_label)\n",
    "                gp = gradient_penalty(self.dis, x_real, x_fake, x_label, device = self.device)\n",
    "                self.d_loss = (-(torch.mean(real_out) - torch.mean(fake_out)) + self.lambda_gp * gp)\n",
    "\n",
    "                self.dis.zero_grad()\n",
    "                self.d_loss.backward()\n",
    "                self.d_opt.step()  \n",
    "                D_losses.append(self.d_loss.data.cpu().numpy())\n",
    "\n",
    "                '''#############        Generator          #######################'''\n",
    "                if (i+1) % n_critic == 0:  \n",
    "                    z_fake = torch.randn(self.batch_size, self.dim_z, 1, 1).to(self.device)\n",
    "                    y_ = []\n",
    "                    for _ in range(self.batch_size): y_.append(random.randrange(0, self.num_class, self.num_class-1))  \n",
    "                    y_ = np.array(y_).reshape([self.batch_size, 1])\n",
    "                    z_label = one_hot(y_, self.num_class).reshape([self.batch_size, self.num_class, 1, 1])\n",
    "                    x_label = z_label * np.ones([self.batch_size, self.num_class, self.dim_height, self.dim_width])\n",
    "                    z_label = torch.Tensor(z_label).to(self.device)    \n",
    "                    x_label = torch.Tensor(x_label).to(self.device)    \n",
    "\n",
    "                    x_fake = self.gen(z_fake, z_label)\n",
    "                    fake_out = self.dis(x_fake, x_label)\n",
    "                    self.g_loss = -torch.mean(fake_out)\n",
    "\n",
    "                    self.g_opt.zero_grad()\n",
    "                    self.g_loss.backward()\n",
    "                    self.g_opt.step()\n",
    "                    G_losses.append(self.g_loss.data.cpu().numpy())\n",
    "\n",
    "\n",
    "            ############        show loss      #######################\n",
    "            if (epoch+1) % 1 == 0:\n",
    "                print('[%d/%d] loss_d: %.3f, loss_g: %.3f'%((epoch + 1), train_epoch, np.mean(D_losses), np.mean(G_losses)))\n",
    "                self.train_hist['D_losses'].append(np.mean(D_losses))\n",
    "                self.train_hist['G_losses'].append(np.mean(G_losses))\n",
    "\n",
    "            ############        visualize generated data      #######################\n",
    "            if (epoch+1) % step_valid == 0:\n",
    "                with torch.no_grad():\n",
    "                   ############        num_alternative = 3      #######################\n",
    "                    z_fake = torch.randn(self.batch_size, self.dim_z, 1, 1).to(self.device)\n",
    "                    z_label = np.zeros([self.batch_size, 1]) + 0\n",
    "                    z_label = one_hot(z_label, self.num_class).reshape([self.batch_size, self.num_class, 1, 1])\n",
    "                    z_label = torch.Tensor(z_label).to(self.device)    \n",
    "\n",
    "                    gen_alt3 = self.gen(z_fake, z_label)\n",
    "                    ############        num_alternative = 4      #######################\n",
    "                    z_fake = torch.randn(self.batch_size, self.dim_z, 1, 1).to(self.device)\n",
    "                    z_label = np.zeros([self.batch_size, 1]) + 1\n",
    "                    z_label = one_hot(z_label, self.num_class).reshape([self.batch_size, self.num_class, 1, 1])\n",
    "                    z_label = torch.Tensor(z_label).to(self.device)    \n",
    "\n",
    "                    gen_alt4 = self.gen(z_fake, z_label)\n",
    "\n",
    "                    plt.figure(epoch)\n",
    "                    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "                    ax.cla()\n",
    "                    ax.imshow(gen_alt3.data.cpu().numpy()[0], cmap='gray')\n",
    "\n",
    "                    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "                    ax.cla()\n",
    "                    ax.imshow(gen_alt4.data.cpu().numpy()[0], cmap='gray')            \n",
    "                    plt.show()      \n",
    "\n",
    "            ############        save per 1000 epoch      #######################\n",
    "            if (epoch+1) % step_save_data == 0:\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    generated_3alt = []\n",
    "                    generated_4alt = []                     \n",
    "                    for _ in range(iteration_generator):  \n",
    "                       ############        num_alternative = 3      #######################\n",
    "                        z_fake = torch.randn(self.batch_size, self.dim_z, 1, 1).to(self.device)\n",
    "                        z_label = np.zeros([self.batch_size, 1]) + 0\n",
    "                        z_label = one_hot(z_label, self.num_class).reshape([self.batch_size, self.num_class, 1, 1])\n",
    "                        z_label = torch.Tensor(z_label).to(self.device)    \n",
    "                        gen_alt3 = self.gen(z_fake, z_label)\n",
    "                        generated_3alt.append(gen_alt3.data.cpu().numpy().reshape([self.batch_size, self.dim_height, self.dim_width]))\n",
    "\n",
    "                        ############        num_alternative = 4      #######################\n",
    "                        z_fake = torch.randn(self.batch_size, self.dim_z, 1, 1).to(self.device)\n",
    "                        z_label = np.zeros([self.batch_size, 1]) + 1\n",
    "                        z_label = one_hot(z_label, self.num_class).reshape([self.batch_size, self.num_class, 1, 1])\n",
    "                        z_label = torch.Tensor(z_label).to(self.device)    \n",
    "                        gen_alt4 = self.gen(z_fake, z_label)\n",
    "                        generated_4alt.append(gen_alt4.data.cpu().numpy().reshape([self.batch_size, self.dim_height, self.dim_width]))\n",
    "\n",
    "                    generated_3alt = np.array(generated_3alt).reshape([iteration_generator*self.batch_size, self.dim_height, self.dim_width])\n",
    "                    gen_alt3_ori = resize_to_ori(generated_3alt, np.math.factorial(6), 30, 24, np.math.factorial(3), self.batch_size, iteration_generator)\n",
    "                    gen_alt3_pd = pd.DataFrame(gen_alt3_ori, columns = ['ABC', 'ACB', 'BAC', 'BCA', 'CAB', 'CBA'])\n",
    "                    gen_alt3_pd.to_csv(generated_path + 'generated_atl3_' + str(epoch) + '.csv')                    \n",
    "\n",
    "                    generated_4alt = np.array(generated_4alt).reshape([iteration_generator*self.batch_size, self.dim_height, self.dim_width])     \n",
    "                    gen_alt4_ori = resize_to_ori(generated_4alt, np.math.factorial(6), 30, 24, np.math.factorial(4), self.batch_size, iteration_generator)\n",
    "                    gen_alt4_pd = pd.DataFrame(gen_alt4_ori, columns = ['ABCD', 'ACBD', 'BACD', 'BCAD', 'CABD', 'CBAD', 'DABC',\n",
    "                           'DACB', 'DBAC', 'DBCA', 'DCAB', 'DCBA', 'ADBC', 'ADCB', 'BDAC', 'BDCA',\n",
    "                           'CDAB', 'CDBA', 'ABDC', 'ACDB', 'BADC', 'BCDA', 'CADB', 'CBDA'])\n",
    "                    gen_alt4_pd.to_csv(generated_path + 'generated_atl4_' + str(epoch) + '.csv')   \n",
    "\n",
    "            time_end_epoch = time.time()\n",
    "            print('Time cost in one epoch', time_end_epoch - time_start_epoch,'s')         \n",
    "\n",
    "        ###########        save      #######################\n",
    "        torch.save(self.gen, saver_path + 'gen.pkl')\n",
    "        torch.save(self.dis, saver_path + 'dis.pkl')\n",
    "        print('save success')    \n",
    "\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        time_end=time.time()\n",
    "        print('Total Time cost',time_end-time_start,'s')\n",
    "        \n",
    "        \n",
    "    '''loss curve'''\n",
    "    def show_train_hist(self):\n",
    "        x = range(len(self.train_hist['D_losses']))\n",
    "\n",
    "        y1 = self.train_hist['D_losses']\n",
    "        y2 = self.train_hist['G_losses']\n",
    "\n",
    "        plt.plot(x, y1, label='D_loss')\n",
    "        plt.plot(x, y2, label='G_loss')\n",
    "\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "\n",
    "        plt.legend(loc=4)  \n",
    "        plt.grid(True)\n",
    "        plt.tight_layout() \n",
    "        plt.title(\"Training Losses\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2de28709",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "model = election_GAN(\n",
    "                num_samples = data_alt_3_4.shape[0],\n",
    "                dim_height = data_alt_3_4.shape[1],\n",
    "                dim_width = data_alt_3_4.shape[2],\n",
    "                dim_z = 128,\n",
    "                num_class = label_alt_onehot.shape[-1],\n",
    "                batch_size = batch_size,\n",
    "                lambda_gp = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3768ea9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization start!\n",
      "[1/2000] loss_d: 1.744, loss_g: -0.892\n",
      "Time cost in one epoch 6.259127616882324 s\n",
      "[2/2000] loss_d: -1.266, loss_g: 0.663\n",
      "Time cost in one epoch 4.071871995925903 s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20000\\3268720954.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mstep_save_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m#1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0miteration_generator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# 2000 samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mn_critic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m#5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     )\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20000\\3012508740.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self, x_train, y_train, dataloder_num_workers, train_epoch, step_valid, step_save_data, iteration_generator, n_critic)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md_opt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m                 \u001b[0mD_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    486\u001b[0m             )\n\u001b[0;32m    487\u001b[0m         torch.autograd.backward(\n\u001b[1;32m--> 488\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m         )\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m def grad(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train_model(\n",
    "    x_train = data_alt_3_4,     \n",
    "    y_train = label_alt_onehot,     \n",
    "    dataloder_num_workers = 1,\n",
    "    train_epoch = 2000,  #17000\n",
    "    step_valid = 25, #50\n",
    "    step_save_data = 1000, #1000\n",
    "    iteration_generator = (2000//batch_size) + 1, # 2000 samples \n",
    "    n_critic = 5, #5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db03b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.show_train_hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c967b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8b7e89",
   "metadata": {},
   "source": [
    "# Show generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fe9b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_trained = torch.load(saver_path + 'gen.pkl')\n",
    "\n",
    "# ############        num_alternative = 3      #######################\n",
    "# z_fake = torch.randn(batch_size, dim_z, 1, 1).to(device)\n",
    "# z_label = np.zeros([batch_size, 1]) + 0\n",
    "# z_label = one_hot(z_label, num_class).reshape([batch_size, num_class, 1, 1])\n",
    "# z_label = torch.Tensor(z_label).to(device)    \n",
    "\n",
    "# gen_alt3 = gen_trained(z_fake, z_label)\n",
    "# ############        num_alternative = 4      #######################\n",
    "# z_fake = torch.randn(batch_size, dim_z, 1, 1).to(device)\n",
    "# z_label = np.zeros([batch_size, 1]) + 1\n",
    "# z_label = one_hot(z_label, num_class).reshape([batch_size, num_class, 1, 1])\n",
    "# z_label = torch.Tensor(z_label).to(device)    \n",
    "\n",
    "# gen_alt4 = gen_trained(z_fake, z_label)\n",
    "\n",
    "# plt.figure(epoch)\n",
    "# fig, ax = plt.subplots(figsize=(3, 3))\n",
    "# ax.cla()\n",
    "# ax.imshow(gen_alt3.data.cpu().numpy()[0], cmap='gray')\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(3, 3))\n",
    "# ax.cla()\n",
    "# ax.imshow(gen_alt4.data.cpu().numpy()[0], cmap='gray')            \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e414c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
